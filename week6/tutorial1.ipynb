{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Social Media & Natural Hazards\n",
    "\n",
    "Natural processes such as thunderstorms, wildfires, earthquakes, and floods may lead to significant losses in terms of property and human life. Gathering information about the damages in time is crucial and may help in mitigating the loss, and faster recovery ([Said et al, 2019](https://link.springer.com/article/10.1007/s11042-019-07942-1)).\n",
    "\n",
    "Social media are one of the most important sources of not only real-time information but records since their existence. They have been crawled over the years to collect and analyze disaster-related multimedia content ([Said et al, 2019](https://link.springer.com/article/10.1007/s11042-019-07942-1)). There are different applications where we can use social media data to analyze natural disasters.\n",
    "\n",
    "Through this tutorial, we will learn how we can use Twitter data to analyze natural hazards. We will do so by applying the concept of natural language processing.\n",
    "\n",
    "\n",
    "\n",
    "This tutorial is heavily based upon the work of [others](https://www.jcchouinard.com/tweepy-basic-functions/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important before we start\n",
    "---\n",
    "Make sure that you save this file before you continue, else you will lose everything. To do so, go to **Bestand/File** and click on **Een kopie opslaan in Drive/Save a Copy on Drive**!\n",
    "\n",
    "Now, rename the file into Week6_Tutorial1.ipynb. You can do so by clicking on the name in the top of this screen.\n",
    "\n",
    "By using this notebook and associated files, you agree to the Twitter Developer Agreement and Policy, which can be found [here](https://developer.twitter.com/en/developer-terms/agreement-and-policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn about the importance and application of social media data\n",
    "- Access social media (Twitter) through the API\n",
    "- Retrieve Twitter data\n",
    "- Filter and clean the retrieved data\n",
    "- Visualize the data in different plots such as `bar`, `scatter`, and `spatial`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h2>Tutorial outline<span class=\"tocSkip\"></span></h2>\n",
    "<hr>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-1\">1. Introduction to the packages</a></span></li><li><span><a href=\"#2.-Basic-Python-Data-Types\" data-toc-modified-id=\"2.-Basic-Python-Data-Types-2\">2. Social Media </a></span></li><li><span><a href=\"#3.-Lists-and-Tuples\" data-toc-modified-id=\"3.-Lists-and-Tuples-3\">3. Natural Language Processing (NLP)</a></span></li><li><span><a href=\"#4.-String-Methods\" data-toc-modified-id=\"4.-String-Methods-4\">4. Data retrieval and post-processing </a></span></li><li><span><a href=\"#5.-Dictionaries\" data-toc-modified-id=\"5.-Dictionaries-5\">5. Applications: detecting natural hazards</a></span></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.Introducing the packages\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this tutorial, we are going to make use of the following packages: \n",
    "\n",
    "[**GeoPandas**](https://geopandas.org/) is a Python packagee that extends the datatypes used by pandas to allow spatial operations on geometric types.\n",
    "\n",
    "[**JSON**](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax.\n",
    "\n",
    "[**Matplotlib**](https://matplotlib.org/) is a comprehensive Python package for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.\n",
    "\n",
    "[**NumPy**](https://numpy.org/doc/stable/) is a Python library that provides a multidimensional array object, various derived objects, and an assortment of routines for fast operations on arrays.\n",
    "\n",
    "[**Pandas**](https://pandas.pydata.org/docs/) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "[**tweepy**](https://www.tweepy.org/) is an easy-to-use Python library for accessing the Twitter API.\n",
    "\n",
    "\n",
    "*We will first need to install these packages in the cell below. Uncomment them to make sure we can pip install them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas\n",
    "#!pip install matplotlib\n",
    "#!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may or may not have seen while installing, there was a warning that we need to restart our runtime. To do so, click on **Runtime** in the topbar menu and click on **Runtime opnieuw starten**/**Restart runtime**.\n",
    "\n",
    "Now we will import these packages in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import geopandas as gpd\n",
    "import tweepy\n",
    "import json\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Social Media\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media are interactive technologies that facilitate the creation and sharing of information, ideas, interests, and other forms of expression through virtual communities and networks.\n",
    "\n",
    "Therefore, social media can be used as a source of real-time information for natural disaster detection. Moreover, the database can be used to post-analyze natural disasters for a better estimation of the extent and the damages the hazard had caused. \n",
    "\n",
    "Some of the most popular social media websites, with more than 100 million registered users, include Facebook (and its associated Facebook Messenger), TikTok, WeChat, ShareChat, Instagram, QZone, Weibo, Twitter, Tumblr, Baidu Tieba, and LinkedIn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter has been proven to be a useful data source for many research communities (Ekta et al, 2017, Graff et al, 2022), from social science to computer science, it can advance research objectives on topics as diverse as the global conversations happening on Twitter. It is one of the most popular online social networking sites with around 450 million monthly [active users](https://www.demandsage.com/twitter-statistics/) as of 2022. An important characteristic of Twitter is its real-time nature.\n",
    "\n",
    "Twitter offers tools and programs that help people when emergencies and natural disasters strike, allowing channels of communication and humanitarian response, among other [areas of focus](https://about.twitter.com/en/who-we-are/twitter-for-good) such as environmental conservation and sustainability. \n",
    "\n",
    "The [Twitter API](https://developer.twitter.com/en) enables programmatic access to Twitter in unique and advanced ways. Twitter's Developer Platform enables you to harness the power of Twitter's open, global, real-time, and historical platform within your own applications. The platform provides tools, resources, data, and API products for you to integrate, and expand Twitter's impact through research, solutions, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use Twitter's Developer Platform we need first a general account, and second to ask for an upgrade to a developer account. \n",
    "\n",
    "Here we show you how to gain access:\n",
    "\n",
    "1.\tHow to sign up on Twitter\n",
    "\n",
    "<img src=\"https://github.com/ElcoK/BigData_AED/blob/main/_static/images/twitter_registration.gif?raw=1\" class=\"bg-primary mb-1\">\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "2.\tHow to get access to [Twitter API](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api)\n",
    "\n",
    "<img src=\"https://github.com/ElcoK/BigData_AED/blob/main/_static/images/twtter_api.gif?raw=1\" class=\"bg-primary mb-1\">\n",
    "<br>\n",
    "\n",
    "Do not forget to copy and save in a safe place your `API key` and `API key Secret`, we will need them later.\n",
    "\n",
    "<img src=\"https://github.com/ElcoK/BigData_AED/blob/main/_static/images/API_Keys.PNG?raw=1\" class=\"bg-primary mb-1\">\n",
    "<br>\n",
    "\n",
    "3.   How to get the tokens from the [Dashboard](https://developer.twitter.com/en/portal/dashboard)\n",
    "\n",
    "<img src=\"https://github.com/ElcoK/BigData_AED/blob/main/_static/images/twitter_token.gif?raw=1\" class=\"bg-primary mb-1\">\n",
    "<br>\n",
    "\n",
    "Do not forget to copy and save in a safe place your `Access Token` and `Access Token Secret`, we will need them later.\n",
    "\n",
    "<img src=\"https://github.com/ElcoK/BigData_AED/blob/main/_static/images/Token.PNG?raw=1\" class=\"bg-primary mb-1\">\n",
    "<br>\n",
    "\n",
    "4.\tHow to upgrade to an elevated access from the [Dashboard](https://developer.twitter.com/en/portal/dashboard)\n",
    "\n",
    "`Please note that for this you will need to verify your account, adding also your phone number`\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/ElcoK/BigData_AED/blob/main/_static/images/twitter_elevated.gif?raw=1\" class=\"bg-primary mb-1\">\n",
    "<br>\n",
    "\n",
    "\n",
    "Note that you can also apply for premium access. For this tutorial, it is not necessary, but if you are interested you can have a look at the [Developer Platform](https://developer.twitter.com/en/docs/twitter-api/premium/search-api/api-reference/premium-search#SearchRequests) to learn about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start exploring `Tweepy`!\n",
    "\n",
    "#### Authenticate with Tweepy\n",
    "\n",
    "In order to gain access to Twitter information we first need to authenticate to the Twitter API.\n",
    "\n",
    "Replace **`Your_info`** with your Twitter credentials that you have obtained in the previous section. \n",
    "\n",
    "<span style=\"color:red\">Maria's API key is used to check if colab works, please do not include them in the final version</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API keyws that yous saved earlier\n",
    "# api_key = \"Your_info\"\n",
    "# api_secrets = \"Your_info\"\n",
    "# access_token = \"Your_info\"\n",
    "# access_secret = \"Your_info\"\n",
    "\n",
    "## Maria's API keys \n",
    "api_key = \"SbjfdyjQgzVKPxfzZEQJtqn5Q\"\n",
    "api_secrets = \"hb1iOFl2hupClPzXn2cI632efGgg4epyA4FDWFZW0eIsyoxtVh\"\n",
    "access_token = \"3996035119-3G6XL2EuVYWShNbi6Lkd516Jb1jb20wpSJDxRPQ\"\n",
    "access_secret = \"ZrbKPyzJdCxHhBnGBLBHrN38TKBFlp3jNpR1o3iYSJqeV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use `tweepy` to authenticate to Twitter.\n",
    " \n",
    "The code includes a verification step to make sure we are successfully authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(api_key,api_secrets)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    " \n",
    "api = tweepy.API(auth)\n",
    " \n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print('Successful Authentication')\n",
    "except:\n",
    "    print('Failed authentication')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting User Information\n",
    "\n",
    "Now that we have successfully authenticated we can make Twitter API requests such as getting the information of a user, for example, we can show the followers of a user or list the user's latest post. \n",
    "\n",
    "So let's try it!\n",
    "\n",
    "The first step is to call the API to get the user that you want to get information from.\n",
    "\n",
    "The get_user() method can take a Twitter screen_name or the user_id as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(screen_name='VU_IVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the data that you can extract for a given user using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a specific field of the data (e.g. name, location, etc.). \n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"user.name: {user.name}\")\n",
    "print(f\"user.screen_name: {user.screen_name}\")\n",
    "print(f\"user.location: {user.location}\")\n",
    "print(f\"user.description: {user.description}\")\n",
    "print(f\"user.followers_count: {user.followers_count}\")\n",
    "print(f\"user.listed_count: {user.listed_count}\")\n",
    "print(f\"user.statuses_count: {user.statuses_count}\")\n",
    "print(f\"user urls: {user.entities['url']['urls'][0]['expanded_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the latest 5 tweets the IVM has posted.\n",
    "\n",
    "The user_timeline() method uses the user_id as a parameter, and we can indicate how many tweets we want using the count parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = user.id_str\n",
    "Latest_tweets = api.user_timeline(user_id=user_id, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you tried to see what the data looks like? \n",
    "\n",
    "You can do it by Uncomment the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, data contains many parameters and its format is not that convenient for analyzing it. \n",
    "\n",
    "Later we will learn how to create a DataFrame and process the data. For now, some important parameters we can extract from the tweet data are the description, location, text, hashtags, among others.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"username: {Latest_tweets[0].user.screen_name}\")\n",
    "print(f\"description: {Latest_tweets[0].user.description}\")\n",
    "print(f\"text: {Latest_tweets[0].text}\")\n",
    "print(f\"date_time: {Latest_tweets[0].created_at}\")\n",
    "print(f\"location: {Latest_tweets[0].user.location}\")\n",
    "print(f\"coordinates: {Latest_tweets[0].coordinates}\")\n",
    "print(f\"following: {Latest_tweets[0].user.friends_count}\")\n",
    "print(f\"followers: {Latest_tweets[0].user.followers_count}\")\n",
    "print(f\"totaltweets: {Latest_tweets[0].user.statuses_count}\")\n",
    "print(f\"retweetcount: {Latest_tweets[0].retweet_count}\")\n",
    "print(f\"hashtags: {Latest_tweets[0].entities['hashtags']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 1:</b> How many followers does @VU_IVM have? \n",
    "When was the last time @VU_IVM tweeted/retweeted?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now learned how to make Twitter API requests to get some information from a specific user. We will continue exploring `tweepy` in the following sections. \n",
    "\n",
    "Please note that we can also make changes to our own account such as updating our profile and interacting with other users. If you're enthusiastic about it, you can find more information [here](https://www.jcchouinard.com/tweepy-basic-functions/) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Natural Language Processing (NLP)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in [Lecture](https://elcok.github.io/BigData_AED/week6/lecture.html), Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interaction between computers and humans in natural language. The ultimate goal of NLP is to help computers understand language as well as we do.\n",
    "There are two ways of understanding natural language: Syntactic and Semantic analysis. Whereas Syntactic analysis (also referred to as syntax analysis or parsing) is the process of analyzing natural language with the rules of formal grammar, Semantic analysis is the process of understanding the meaning and interpretation of words, signs, and sentence structure.\n",
    "\n",
    "There are different techniques for understanding text such as Parsing, Stemming, Text Segmentation, Named Entity Recognition, Relationship Extraction, and Sentiment Analysis (see [Lecture](https://elcok.github.io/BigData_AED/week6/lecture.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start here by analyzing a user's tweets by simply looking into the words he/she uses the most. \n",
    "\n",
    "Barack Obama ([@BarackObama](https://twitter.com/BarackObama)) has the Twitter account with the most followers according to [this website](https://www.tweetbinder.com/blog/top-twitter-accounts/). So let's explore the words he uses the most. \n",
    "\n",
    "First, we will retrieve his last 200 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(screen_name='BarackObama')\n",
    "user_id = user.id_str\n",
    "\n",
    "Latest_tweets = api.user_timeline(user_id=user_id, count=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we learned how to access get the text of each tweet by using .text so let's go further first with only one tweet. \n",
    "\n",
    "What type of data does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Latest_tweets[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before analyzing @BarackObama tweets, we need to split the string text into each word to be able to count them.\n",
    "\n",
    "The following function is ready to use, the input is the string and the output is a dictionary that contains each word and how the count for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the function is working, you can play around with different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Big Data Analysis is really fun!'\n",
    "print(word_count(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready now to analyze the data.\n",
    "\n",
    "We will first merge the texts of all the retrieved tweets, and subsequently, we will use the word_count function to count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_text = ''\n",
    "for i in range(len(Latest_tweets)):\n",
    "    concat_text = concat_text +\" \"+ Latest_tweets[i].text.lower()\n",
    "concat_text\n",
    "words = word_count(concat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many words @BarackObama has used. \n",
    "\n",
    "You can also see what the data looks like by Uncomment the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{user.screen_name} has used {len(words)} different words')\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the data in a plot bar using the following lines. \n",
    "\n",
    "Note that a filter is added to only visualize the words that have been used more than 5 times. You can change the count_filter to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter = 5\n",
    "\n",
    "x = []\n",
    "h = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    if list(words.items())[i][1] > count_filter:\n",
    "        x.append(list(words.items())[i][0])\n",
    "        h.append(list(words.items())[i][1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5)) \n",
    "plt.bar(x, h)\n",
    "plt.xticks(fontsize = 10, rotation = 70)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.title(f'The most used words by {user.screen_name} in the last 200 tweets\\n', fontsize = 20) \n",
    "plt.ylabel(f'Count', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that many words are prepositions? \n",
    "\n",
    "If we desire to determine more relevant words to know more about the context of the tweets we need to further filter the data. \n",
    "\n",
    "There are NLP packages that allow not only clustering analysis but also semantic analysis. So we can also evaluate whether the tweets have a positive or negative connotation. If you are interested you can have a look at [`NLTK`](https://www.nltk.org/api/nltk.tag.api.html).\n",
    "\n",
    "Here we will apply our own word filter that includes prepositions and other words that will help to have a nicer plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = ['aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'around', 'as', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "        'beside', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'following', 'for', 'from', 'in', \n",
    "        'inside', 'into', 'like', 'minus', 'near', 'next', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', \n",
    "        'regarding', 'round', 'save', 'since', 'than', 'through', 'till', 'to', 'toward', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', \n",
    "        'via', 'with', 'within', 'without', 'and', 'the', 'a', 'this', 'rt', 'can', 'have', 'if', 'you', 'i', 'some', 'was', 'we', 'our', 'be', 'who',\n",
    "        'this', 'rt', 'can', 'have', 'if', 'you', 'i', 'some', 'was', 'we', 'our', 'be', 'who', 'is', 'my', '–', 'are', 'know','that', 'will', 'get',\n",
    "        'make', 'your', 'more', 'so', 'don’t', 'it’s', 'i’m', 'an', 'their', 'it', 'us', 'his', 'not', 'one', 'what', 'when', 'first', 'sure', 'do', 'new', 'last',\n",
    "        'all', 'many', 'just', 'been', 'want', 'only', 'years', 'year', 'today', 'day']\n",
    "\n",
    "count_filter = 5\n",
    "x = []\n",
    "h = []\n",
    "filter = []\n",
    "for i in range(len(words)):\n",
    "    if list(words.items())[i][1] > count_filter:\n",
    "        w = list(words.items())[i][0]\n",
    "        c = 0\n",
    "        for p in word_filter:\n",
    "            if w == p:\n",
    "                c = c + 1\n",
    "\n",
    "        if c == 0:\n",
    "            x.append(list(words.items())[i][0])\n",
    "            h.append(list(words.items())[i][1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5)) \n",
    "plt.bar(x, h)\n",
    "plt.xticks(fontsize = 10, rotation = 70)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.title(f'The most used words by {user.screen_name} in the last 200 tweets\\nUsing a word filter', fontsize = 20) \n",
    "plt.ylabel(f'Count', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 2:</b> Upload the filtered figure of the most used words by @BarackObama in the last 200 tweets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 3:</b> Which is the word that he uses the most? What is the context/tone of his words? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data retrieval and post-processing\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned how to retrieve the tweets of a specific user. \n",
    "Now it is time to retrieve them by using keywords that can be content in the text such as hashtags. \n",
    "That will allow us to analyze what is happening at a specific time and/or location. \n",
    "\n",
    "We'll start using the tweepy Cursor method to look for the latest 10 tweets that contain the word \"Earthquake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Colab uses api.search NOT api.search_tweets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab uses api.search NOT api.search_tweets\n",
    "tweets = tweepy.Cursor(api.search_tweets, 'Earthquake', lang=\"en\", tweet_mode='extended').items(100)\n",
    "list_tweets = [tweet for tweet in tweets]\n",
    "len(list_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the data looks like by Uncomment the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, it is difficult to read the tweets, and therefore, in order to do an analysis we first need to process the data. \n",
    "\n",
    "We know how to extract specific fields from the data, it would be handy to create then a DataFrame with the information we need.\n",
    "\n",
    "The following function uses the list of tweets to create a DataFrame, extract the information of each tweet, and finally add it to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFrame_tweets(list_tweets):\n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                'description',\n",
    "                                'date_time',\n",
    "                                'location',\n",
    "                                'following',\n",
    "                                'followers',\n",
    "                                'totaltweets',\n",
    "                                'retweetcount',\n",
    "                                'text',\n",
    "                                'hashtags'])\n",
    "\n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                date_time = tweet.created_at\n",
    "                location = tweet.user.location\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    "\n",
    "        # Retweets can be distinguished by\n",
    "        # a retweeted_status attribute,\n",
    "        # in case it is an invalid reference,\n",
    "        # except block will be executed\n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.full_text\n",
    "                hashtext = list()\n",
    "                for j in range(0, len(hashtags)):\n",
    "                        hashtext.append(hashtags[j]['text'])\n",
    "                \n",
    "                                \n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [username, description, date_time,\n",
    "                        location, following, \n",
    "                        followers, totaltweets,\n",
    "                        retweetcount, text, hashtext]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our DataFrame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataFrame_tweets(list_tweets)\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a minute to look at the information about the location.\n",
    "\n",
    "As you may notice, not all the users share the location, and some of the users that do share it, do not necessarily use a real location. \n",
    "\n",
    "We can also obtain the geographical location by coordinates, let's try and find out if there is more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFrame_tweets_coordinates(list_tweets):\n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                'description',\n",
    "                                'date_time',\n",
    "                                'location',\n",
    "                                'Coordinates',\n",
    "                                'following',\n",
    "                                'followers',\n",
    "                                'totaltweets',\n",
    "                                'retweetcount',\n",
    "                                'text',\n",
    "                                'hashtags'])\n",
    "\n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                date_time = tweet.created_at\n",
    "                location = tweet.user.location\n",
    "                coordinates = tweet.coordinates\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    "\n",
    "        # Retweets can be distinguished by\n",
    "        # a retweeted_status attribute,\n",
    "        # in case it is an invalid reference,\n",
    "        # except block will be executed\n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.full_text\n",
    "                hashtext = list()\n",
    "                for j in range(0, len(hashtags)):\n",
    "                        hashtext.append(hashtags[j]['text'])\n",
    "                \n",
    "                                \n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [username, description, date_time,\n",
    "                        location, coordinates, following,\n",
    "                        followers, totaltweets,\n",
    "                        retweetcount, text, hashtext]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataFrame_tweets_coordinates(list_tweets)\n",
    "# db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the tweets that contain the geographical location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(db)):\n",
    "    if db.Coordinates[i] != None:\n",
    "        count += 1\n",
    "\n",
    "print(f'{count} tweets contain the coordinates out of the 100 retrieved tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 4:</b> How many tweets contain the coordinates? \n",
    "Mention at least one advantage and one disadvantage of not sharing the coordinates.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately not all (or none) of the users share the real location nor allow the geolocation for the coordinates.\n",
    "\n",
    "Therefore, when we want to analyze a certain region we can't use all the tweets and we need to further filter the information.\n",
    "\n",
    "The following function will filter the tweets and only save the tweets that contain coordinates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Colab uses api.search NOT api.search_tweets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(words, numtweet):\n",
    " \n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                   'description',\n",
    "                                   'date_time',\n",
    "                                   'location',\n",
    "                                   'Coordinates',\n",
    "                                   'X',\n",
    "                                   'Y',\n",
    "                                   'following',\n",
    "                                   'followers',\n",
    "                                   'totaltweets',\n",
    "                                   'retweetcount',\n",
    "                                   'text',\n",
    "                                   'hashtags'])\n",
    " \n",
    "        # We are using .Cursor() to search\n",
    "        # through twitter for the required tweets.\n",
    "        # The number of tweets can be\n",
    "        # restricted using .items(number of tweets)\n",
    " # Colab uses api.search NOT api.search_tweets       \n",
    "        tweets = tweepy.Cursor(api.search_tweets,\n",
    "                               words, lang=\"en\",\n",
    "                               tweet_mode='extended').items(numtweet)\n",
    " \n",
    " \n",
    "        # .Cursor() returns an iterable object. Each item in\n",
    "        # the iterator has various attributes that you can \n",
    "        # access to get information about each tweet\n",
    "        list_tweets = [tweet for tweet in tweets]\n",
    " \n",
    "        i = 1\n",
    " \n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                date_time = tweet.created_at\n",
    "                location = tweet.user.location\n",
    "                coordinates = tweet.coordinates\n",
    "                x=0\n",
    "                y=0\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    " \n",
    "                # Retweets can be distinguished by\n",
    "                # a retweeted_status attribute,\n",
    "                # in case it is an invalid reference,\n",
    "                # except block will be executed\n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.full_text\n",
    "                hashtext = list()\n",
    "                for j in range(0, len(hashtags)):\n",
    "                        hashtext.append(hashtags[j]['text'])\n",
    "                \n",
    "                # Only considering the tweets with \n",
    "                # coordinates\n",
    "                if coordinates == None: \n",
    "                        i = i+1\n",
    "                else:         \n",
    "                        # Here we are appending all the\n",
    "                        # extracted information in the DataFrame\n",
    "                        ith_tweet = [username, description, date_time,\n",
    "                                location, coordinates, x, y,  following,\n",
    "                                followers, totaltweets,\n",
    "                                retweetcount, text, hashtext]\n",
    "                        db.loc[len(db)] = ith_tweet\n",
    "                        i = i+1\n",
    "\n",
    "        # Coordinates post-processing \n",
    "        # X & Y coordinates\n",
    "        for i in range(len(db)):\n",
    "                db['X'][i] = db.Coordinates.loc[i]['coordinates'][0]\n",
    "                db['Y'][i] = db.Coordinates.loc[i]['coordinates'][1]\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = scrape('Earthquake', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application: Natural Hazards\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous section, we started filtering the tweets by keywords and location. \n",
    "\n",
    "We'll continue with the earthquake example.\n",
    "\n",
    "Did you notice there is a user that uses the [USGS](https://www.usgs.gov/programs/earthquake-hazards/earthquakes) as a source of its tweets?\n",
    "\n",
    "That's right, it is 'everyEarthquake'.  \n",
    "\n",
    "So this time we'll retrieve the tweets by a user using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user(uID, numtweet):  \n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                   'description',\n",
    "                                   'date_time',\n",
    "                                   'location',\n",
    "                                   'Coordinates',\n",
    "                                   'X', 'Y',\n",
    "                                   'following',\n",
    "                                   'followers',\n",
    "                                   'totaltweets',\n",
    "                                   'retweetcount',\n",
    "                                   'text',\n",
    "                                   'hashtags'])\n",
    " \n",
    "        # Retrieving tweets by user\n",
    "        list_tweets = api.user_timeline(id='everyEarthquake', count=numtweet)\n",
    "        \n",
    "        \n",
    "        i = 1\n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                date_time = tweet.created_at\n",
    "                location = tweet.user.location\n",
    "                coordinates = tweet.coordinates\n",
    "                x, y = 0, 0\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    " \n",
    "                # Retweets can be distinguished by\n",
    "                # a retweeted_status attribute,\n",
    "                # in case it is an invalid reference,\n",
    "                # except block will be executed\n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.text\n",
    "                hashtext = list()\n",
    "                for j in range(0, len(hashtags)):\n",
    "                        hashtext.append(hashtags[j]['text'])\n",
    "                \n",
    "                # Only considering the tweets with \n",
    "                # coordinates\n",
    "                if coordinates == None: \n",
    "                        i = i+1\n",
    "                else:\n",
    "                         \n",
    "                        # Here we are appending all the\n",
    "                        # extracted information in the DataFrame\n",
    "                        ith_tweet = [username, description, date_time,\n",
    "                                location, coordinates, x, y, following,\n",
    "                                followers, totaltweets,\n",
    "                                retweetcount, text, hashtext]\n",
    "                        db.loc[len(db)] = ith_tweet\n",
    "                        i = i+1\n",
    "\n",
    "        # Coordinates post-processing \n",
    "        # X & Y coordinates\n",
    "        for i in range(len(db)):\n",
    "                db['X'][i] = db.Coordinates.loc[i]['coordinates'][0]\n",
    "                db['Y'][i] = db.Coordinates.loc[i]['coordinates'][1]\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the last 100 posts made by @everyEarthquake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = user('everyEarthquake', 100)\n",
    "# db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use GeoPandas to plot the location of the last 100 earthquakes. \n",
    "\n",
    "First, we will convert our DataFrame to a GeoDataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = gpd.GeoDataFrame(db, geometry=gpd.points_from_xy(db.X, db.Y))\n",
    "# dbg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax.set_title('Some earthquakes reported on Twitter', fontsize=22)\n",
    "\n",
    "world.boundary.plot(ax=ax, color='k', alpha=.3)\n",
    "dbg.plot(ax=ax, marker='o', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that the text contains information about the magnitude of earthquakes?\n",
    "\n",
    "We can also indicate the magnitude in our plot. \n",
    "\n",
    "We need to process the information to extract the values from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg['magnitude'] = 0\n",
    "for i in range(len(db)):\n",
    "    text = dbg.text.loc[i]\n",
    "    text_split= text.split(\" \")\n",
    "    Mag = float(text_split[3].replace('M', ''))\n",
    "    dbg['magnitude'][i] = Mag\n",
    "    \n",
    "# dbg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg.loc[dbg['magnitude'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the plot looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot size of the circle\n",
    "z = dbg.magnitude\n",
    "\n",
    "#Plot color of the circle\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax.set_title('Some earthquakes reported on Twitter', fontsize=22)\n",
    "\n",
    "world.boundary.plot(ax=ax, color='k', alpha=.3)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "dbg.plot('magnitude', ax=ax, marker='o', markersize=z*30, edgecolors='k' , cmap='YlOrRd',vmin=0, vmax=8, zorder=2, legend=True, legend_kwds={'label': f\"Magnitude\", 'orientation': \"vertical\"}, cax=cax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 5:</b> Upload the map showing the reported earthquakes, showing the magnitude of the events.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 6:</b> Where and when happen the earthquake with the highest magnitude? Which is its magnitude?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the map from the [USGS website](https://earthquake.usgs.gov/earthquakes/map/?extent=-68.0733,-194.23828&extent=77.15716,199.51172&listOnlyShown=true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 7:</b> Do you see differences from the figure that you have generated?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have mentioned, users do not always share the location or the coordinates. \n",
    "\n",
    "However, there are other applications where we can still use the tweets without the location. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 8:</b> Can you think of examples of other applications? Mention at least one application that needs coordinates and one that does not need coordinates.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flooding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example of floods. This time we will use a database that has been already downloaded. \n",
    "The database contains tweets about floods located in Texas from 30/07/2014 to 15/11/2022\n",
    "\n",
    "```{tip} \n",
    "We're going to use the file 'tweets.jsonl', make sure to save it in the same folder than the notebook.\n",
    "\n",
    "```\n",
    "\n",
    "We can read the data using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Needed for Colab: # !gdown 11QfCIMHOjcgNvWMFsZTk5btXgYiRCXRq</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for Colab:\n",
    "# !gdown 11QfCIMHOjcgNvWMFsZTk5btXgYiRCXRq\n",
    "\n",
    "with open('tweets.jsonl') as f:\n",
    "    tweets = [json.loads(line) for line in f]\n",
    "for tweet in tweets:\n",
    "    tweet['text'] = tweet['text'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets[:10]:\n",
    "    print(tweet['date'], '-', tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot this data as a bar plot to identify the days when more tweets have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = date(2014, 7, 30)\n",
    "END_DATE = date(2022, 11, 15)\n",
    "\n",
    "def plot_tweets(tweets, title):\n",
    "    dates = [tweet['date'] for tweet in tweets]\n",
    "    dates = [datetime.fromisoformat(date) for date in dates]\n",
    "    plt.figure(figsize=(10, 5)) \n",
    "    plt.hist(dates, range=(START_DATE, END_DATE), bins=(END_DATE - START_DATE).days)\n",
    "    plt.xticks(fontsize = 10)\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.title(f'{title}', fontsize = 16) \n",
    "    plt.ylabel(f'Count', fontsize=12)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tweets(tweets, 'Flood histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 9:</b> Upload the flood histogram.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 10:</b> When does the high peak usually happen each year? What could be a potential explanation for the pattern?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 11:</b> When does the bigger peak occur? What was the cause of it?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Process (NLP), semantic analysis is the process of understanding the meaning and interpretation of words. \n",
    "\n",
    "This time we can use keywords to filter the tweets, identifying negative or positive meanings.\n",
    "\n",
    "We're starting with negative words such as 'cry' and 'warning':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_keywords = ['cry', 'warning']\n",
    "filtered_tweets = []\n",
    "for tweet in tweets:\n",
    "    if not any(keyword in tweet['text'] for keyword in negative_keywords):\n",
    "        filtered_tweets.append(tweet)\n",
    "\n",
    "print(len(tweets))\n",
    "print(len(filtered_tweets))\n",
    "plot_tweets(filtered_tweets, 'Flood histogram \\nNegative keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try positive keywords such as 'emergency' and 'rescue':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_keywords = ['emergency', 'rescue']\n",
    "filtered_tweets = []\n",
    "for tweet in tweets:\n",
    "    if any(keyword in tweet['text'] for keyword in positive_keywords):\n",
    "        filtered_tweets.append(tweet)\n",
    "\n",
    "print(len(tweets))\n",
    "print(len(filtered_tweets))\n",
    "plot_tweets(filtered_tweets, 'Flood histogram \\nPositive keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 12:</b> Can you think about other negative keywords? Can you think of other positive keywords?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lecture Outline",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "ad67672930c25b4e013d711121a8401d6e445a01ee16ba39e2701f0a12a7bf96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
