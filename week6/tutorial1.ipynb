{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnMAMUx66-9C"
   },
   "source": [
    "# Tutorial 1: Social Media & Natural Hazards\n",
    "\n",
    "Natural processes such as thunderstorms, wildfires, earthquakes, and floods may lead to significant losses in terms of property and human life. Gathering information about the damages in time is crucial and may help in mitigating the loss, and faster recovery ([Said et al, 2019](https://link.springer.com/article/10.1007/s11042-019-07942-1)).\n",
    "\n",
    "Social media are one of the most important sources of not only real-time information but records since their existence. They have been crawled over the years to collect and analyze disaster-related multimedia content ([Said et al, 2019](https://link.springer.com/article/10.1007/s11042-019-07942-1)). There are different applications where we can use social media data to analyze natural disasters.\n",
    "\n",
    "Through this tutorial, we will learn how we can use Twitter data to analyze natural hazards. We will do so by applying the concept of natural language processing.\n",
    "\n",
    "\n",
    "\n",
    "This tutorial is heavily based upon the work of [others](https://www.jcchouinard.com/tweepy-basic-functions/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfI4fylf6-9F"
   },
   "source": [
    "### Important before we start\n",
    "---\n",
    "Make sure that you save this file before you continue, else you will lose everything. To do so, go to **Bestand/File** and click on **Een kopie opslaan in Drive/Save a Copy on Drive**!\n",
    "\n",
    "Now, rename the file into Week6_Tutorial1.ipynb. You can do so by clicking on the name in the top of this screen.\n",
    "\n",
    "By using this notebook and associated files, you agree to the Twitter Developer Agreement and Policy, which can be found [here](https://developer.twitter.com/en/developer-terms/agreement-and-policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR69C3_d6-9I"
   },
   "source": [
    "## Learning Objectives \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFosroWc6-9J"
   },
   "source": [
    "- Learn about the importance and application of social media data\n",
    "- Access social media (Twitter) through the API\n",
    "- Retrieve Twitter data\n",
    "- Filter and clean the retrieved data\n",
    "- Visualize the data in different plots such as `bar`, `scatter`, and `spatial`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXY3uirp6-9K",
    "toc": true
   },
   "source": [
    "<h2>Tutorial outline<span class=\"tocSkip\"></span></h2>\n",
    "<hr>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#introducing-the-packages\" data-toc-modified-id=\"1.-Introduction-1\">1. Introducing the packages</a></span></li>\n",
    "    <li><span><a href=\"#social-media\" data-toc-modified-id=\"2.-Basic-Python-Data-Types-2\">2. Social Media </a></span></li>\n",
    "    <li><span><a href=\"#natural-language-processing-(nlp)\" data-toc-modified-id=\"3.-Lists-and-Tuples-3\">3. Natural Language Processing (NLP)</a></span></li>\n",
    "    <li><span><a href=\"#data-retrieval-and-post-processing\" data-toc-modified-id=\"4.-String-Methods-4\">4. Data retrieval and post-processing </a></span></li>\n",
    "    <li><span><a href=\"#applications:-detecting-natural-hazards\" data-toc-modified-id=\"5.-Dictionaries-5\">5. Applications: detecting natural hazards</a></span></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuLiygDe6-9K",
    "tags": []
   },
   "source": [
    "## 1.Introducing the packages\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVsafgLj6-9K"
   },
   "source": [
    "Within this tutorial, we are going to make use of the following packages: \n",
    "\n",
    "[**GeoPandas**](https://geopandas.org/) is a Python packagee that extends the datatypes used by pandas to allow spatial operations on geometric types.\n",
    "\n",
    "[**JSON**](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax.\n",
    "\n",
    "[**Matplotlib**](https://matplotlib.org/) is a comprehensive Python package for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.\n",
    "\n",
    "[**NumPy**](https://numpy.org/doc/stable/) is a Python library that provides a multidimensional array object, various derived objects, and an assortment of routines for fast operations on arrays.\n",
    "\n",
    "[**Pandas**](https://pandas.pydata.org/docs/) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "[**tweepy**](https://www.tweepy.org/) is an easy-to-use Python library for accessing the Twitter API.\n",
    "\n",
    "\n",
    "*We will first need to install these packages in the cell below. Uncomment them to make sure we can pip install them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTEyjbOh6-9L",
    "outputId": "201a1316-1af9-4d78-c5e6-9b638b5f7139"
   },
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQLuc04_6-9M"
   },
   "source": [
    "As you may or may not have seen while installing, there was a warning that we need to restart our runtime. To do so, click on **Runtime** in the topbar menu and click on **Runtime opnieuw starten**/**Restart runtime**.\n",
    "\n",
    "Now we will import these packages in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBTL4t5A6-9N"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import geopandas as gpd\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luWgRZ9V6-9N"
   },
   "source": [
    "## 2. Social Media\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGpSH17J6-9N"
   },
   "source": [
    "Social media are interactive technologies that facilitate the creation and sharing of information, ideas, interests, and other forms of expression through virtual communities and networks.\n",
    "\n",
    "Therefore, social media can be used as a source of real-time information for natural disaster detection. Moreover, the database can be used to post-analyze natural disasters for a better estimation of the extent and the damages the hazard had caused. \n",
    "\n",
    "Some of the most popular social media websites, with more than 100 million registered users, include Facebook (and its associated Facebook Messenger), TikTok, WeChat, ShareChat, Instagram, QZone, Weibo, Twitter, Tumblr, Baidu Tieba, and LinkedIn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB_Zsdwm6-9O"
   },
   "source": [
    "Twitter has been proven to be a useful data source for many research communities (Ekta et al, 2017, Graff et al, 2022), from social science to computer science, it can advance research objectives on topics as diverse as the global conversations happening on Twitter. It is one of the most popular online social networking sites with around 450 million monthly [active users](https://www.demandsage.com/twitter-statistics/) as of 2022. An important characteristic of Twitter is its real-time nature.\n",
    "\n",
    "Twitter offers tools and programs that help people when emergencies and natural disasters strike, allowing channels of communication and humanitarian response, among other [areas of focus](https://about.twitter.com/en/who-we-are/twitter-for-good) such as environmental conservation and sustainability. \n",
    "\n",
    "The [Twitter API](https://developer.twitter.com/en) enables programmatic access to Twitter in unique and advanced ways. Twitter's Developer Platform enables you to harness the power of Twitter's open, global, real-time, and historical platform within your own applications. The platform provides tools, resources, data, and API products for you to integrate, and expand Twitter's impact through research, solutions, and more.\n",
    "\n",
    "Unfortunately, Twitter has made their API policy very strict since February 12. As such, we cannot use their API within this tutorial. As such, we will use data that was previously retrieved by us (in the preparation of this tutorial). \n",
    "\n",
    "Download the 'Week6_Data' folder provided in Canvas and save it to your previously created BigData folder on your Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6PYNLTR6-9O"
   },
   "source": [
    "### Connect to google drive\n",
    "<hr>\n",
    "\n",
    "To be able to read the data from Google Drive, we need to *mount* our Drive to this notebook.\n",
    "\n",
    "As you can see in the cell below, make sure that in your **My Drive** folder, where you created **BigData** folder and within that folder, you have created a **Week6_Data** folder in which you can store the files that are required to run this analysis.\n",
    "\n",
    "Please go the URL when its prompted in the box underneath the following cell, and copy the authorization code in that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnI0VgHY6-9O",
    "outputId": "a99bb999-2567-4d94-d78a-675b4581e101"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "sys.path.append(\"/content/gdrive/My Drive/BigData/Week6_Data\")\n",
    "\n",
    "data_path = os.path.join('/content/gdrive/My Drive/BigData','Week6_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22kJ7CZt6-9P"
   },
   "source": [
    "#### User Information\n",
    "\n",
    "Trough Twitter API we can make requests such as getting the information of a user, for example, we can show the followers of a user or list the user's latest post. \n",
    "\n",
    "The following line will open the previously retrieved information of the [IVM - VU](https://twitter.com/VU_IVM) account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2zXZEYB6-9P"
   },
   "outputs": [],
   "source": [
    "user_file = os.path.join(data_path, r'VU_user.jsonl') \n",
    "\n",
    "f = open(user_file)\n",
    "user = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyUHLlHL6-9P"
   },
   "source": [
    "Have look at the information provided by Twitter for a specific user using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXUkM9Ms6-9P",
    "outputId": "15278d50-ca09-4cdc-b3c2-e289b99ecd43"
   },
   "outputs": [],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6O7JFNu6-9P"
   },
   "source": [
    "We can also look at a specific field of the data (e.g. name, location, etc.). \n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "WgTjRy6A6-9P",
    "outputId": "21923288-31f1-492b-b5a6-0c12a592f534"
   },
   "outputs": [],
   "source": [
    "print(f\"user.name: {user['name']}\")\n",
    "print(f\"user.screen_name: {user['screen_name']}\")\n",
    "print(f\"user.location: {user['XXXX']}\")\n",
    "print(f\"user.description: {user['XXXX']}\")\n",
    "print(f\"user.followers_count: {user['XXXX']}\")\n",
    "print(f\"user.listed_count: {user['listed_count']}\")\n",
    "print(f\"user.statuses_count: {user['statuses_count']}\")\n",
    "print(f\"user urls: {user['entities']['url']['urls'][0]['expanded_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CW0TSdD6-9Q"
   },
   "source": [
    "Now let's see the latest 5 tweets the IVM has posted. We will use the data that was previously retrieved:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPgNT6xE6-9Q"
   },
   "outputs": [],
   "source": [
    "tweets_file = os.path.join(data_path, r'VU_tweets.jsonl') \n",
    "\n",
    "with open(tweets_file) as f:\n",
    "    tweets = [json.loads(line) for line in f]\n",
    "\n",
    "Latest_tweets = tweets[0]\n",
    "# f = open(user_file)\n",
    "# user = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTadNR826-9Q"
   },
   "source": [
    "Have you tried to see what the data looks like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DM-o2DM6-9Q",
    "outputId": "24688f56-8936-4bba-c0d6-7720a1437c7e"
   },
   "outputs": [],
   "source": [
    "Latest_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjtE8VMP6-9Q"
   },
   "source": [
    "As you can see, data contains many parameters and its format is not that convenient for analyzing it. \n",
    "\n",
    "Later we will learn how to create a DataFrame and process the data. For now, some important parameters we can extract from the tweet data are the description, location, text, hashtags, among others.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYVH6gG06-9Q"
   },
   "outputs": [],
   "source": [
    "print(f\"username: {Latest_tweets[0]['screen_name']}\")\n",
    "print(f\"description: {Latest_tweets[0]['XXXXX']}\")\n",
    "print(f\"text: {Latest_tweets[0]['XXXX']}\")\n",
    "print(f\"date_time: {Latest_tweets[0]['created_at']}\")\n",
    "print(f\"location: {Latest_tweets[0]['XXXX']}\")\n",
    "print(f\"coordinates: {Latest_tweets[0]['XXXX']}\")\n",
    "print(f\"following: {Latest_tweets[0]['friends_count']}\")\n",
    "print(f\"followers: {Latest_tweets[0]['followers_count']}\")\n",
    "print(f\"totaltweets: {Latest_tweets[0]['statuses_count']}\")\n",
    "print(f\"retweetcount: {Latest_tweets[0]['retweet_count']}\")\n",
    "print(f\"hashtags: {Latest_tweets[0]['hashtags']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHgjAL5y6-9R"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 1:</b> How many followers does @VU_IVM have? \n",
    "When was the last time @VU_IVM tweeted/retweeted?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRUnCR8L6-9R"
   },
   "source": [
    "We've now learned how to make Twitter API requests (albeit unconnected) to get some information from a specific user. \n",
    "\n",
    "Please note that we can also make changes to our own account such as updating our profile and interacting with other users. If you're enthusiastic about it, you can find more information [here](https://www.jcchouinard.com/tweepy-basic-functions/) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k1lOWrF6-9R"
   },
   "source": [
    "## 3. Natural Language Processing (NLP)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VPhByiq6-9R"
   },
   "source": [
    "As mentioned in [Lecture](https://elcok.github.io/BigData_AED/week6/lecture.html), Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interaction between computers and humans in natural language. The ultimate goal of NLP is to help computers understand language as well as we do.\n",
    "There are two ways of understanding natural language: Syntactic and Semantic analysis. Whereas Syntactic analysis (also referred to as syntax analysis or parsing) is the process of analyzing natural language with the rules of formal grammar, Semantic analysis is the process of understanding the meaning and interpretation of words, signs, and sentence structure.\n",
    "\n",
    "There are different techniques for understanding text such as Parsing, Stemming, Text Segmentation, Named Entity Recognition, Relationship Extraction, and Sentiment Analysis (see [Lecture](https://elcok.github.io/BigData_AED/week6/lecture.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0_17GRZ6-9R"
   },
   "source": [
    "We can start here by analyzing a user's tweets by simply looking into the words he/she uses the most. \n",
    "\n",
    "Barack Obama ([@BarackObama](https://twitter.com/BarackObama)) has the Twitter account with the most followers according to [this website](https://www.tweetbinder.com/blog/top-twitter-accounts/). So let's explore the words he uses the most. \n",
    "\n",
    "We retrieved his last 200 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VHi0hf56-9R"
   },
   "outputs": [],
   "source": [
    "user_file = os.path.join(data_path, r'Obama_user.jsonl') \n",
    "\n",
    "f = open(user_file)\n",
    "user = json.load(f)\n",
    "\n",
    "tweets_file = os.path.join(data_path, r'Obama_tweets.jsonl') \n",
    "\n",
    "with open(tweets_file) as f:\n",
    "    tweets = [json.loads(line) for line in f]\n",
    "\n",
    "Latest_tweets = tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUtxGs9j6-9R"
   },
   "source": [
    "In the previous section, we learned how to access get the **text** of each tweet by using ['text'] so let's go further first with only one tweet. \n",
    "\n",
    "What type of data does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-MH6oda6-9R",
    "outputId": "f2b6d24e-be85-4837-e136-fc8724c5d773"
   },
   "outputs": [],
   "source": [
    "type(Latest_tweets[0]['screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1hHHpmB6-9S"
   },
   "source": [
    "Before analyzing @BarackObama tweets, we need to split the string text into each word to be able to count them.\n",
    "\n",
    "The following function is ready to use, the input is the string and the output is a dictionary that contains each word and how the count for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqu4IX-U6-9S"
   },
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYcf6S-66-9S"
   },
   "source": [
    "Let's make sure the function is working, you can play around with different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVa3bQQ46-9S",
    "outputId": "323b514e-03f1-4a82-e764-0282012e2c21"
   },
   "outputs": [],
   "source": [
    "sentence = 'Big Data Analysis is really fun!'\n",
    "print(word_count(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa0JxybC6-9S"
   },
   "source": [
    "We are ready now to analyze the data.\n",
    "\n",
    "We will first merge the texts of all the retrieved tweets, and subsequently, we will use the word_count function to count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03k4s72p6-9S"
   },
   "outputs": [],
   "source": [
    "concat_text = ''\n",
    "for i in range(len(Latest_tweets)):\n",
    "    concat_text = concat_text +\" \"+ Latest_tweets[i]['text'].lower()\n",
    "concat_text\n",
    "words = word_count(concat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnB0xgbl6-9S"
   },
   "source": [
    "Let's see how many words @BarackObama has used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npITJZbK6-9S",
    "outputId": "f5bc0e07-1783-4679-8f59-4b32b033f3cc"
   },
   "outputs": [],
   "source": [
    "screen_name = user['screen_name']\n",
    "print(f'{screen_name} has used {len(words)} different words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G2s1E3q6-9T"
   },
   "source": [
    "We can visualize the data in a plot bar using the following lines. \n",
    "\n",
    "Note that a filter is added to only visualize the words that have been used more than 5 times. You can change the count_filter to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "XKo49_606-9T",
    "outputId": "3c4c0fbe-1290-4def-e6d2-136b79c4c338"
   },
   "outputs": [],
   "source": [
    "count_filter = 5\n",
    "\n",
    "x = []\n",
    "h = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    if list(words.items())[i][1] > count_filter:\n",
    "        x.append(list(words.items())[i][0])\n",
    "        h.append(list(words.items())[i][1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5)) \n",
    "plt.bar(x, h)\n",
    "plt.xticks(fontsize = XX, rotation = XX)\n",
    "plt.yticks(fontsize = XX)\n",
    "plt.title(f'The most used words by {screen_name} in the last 200 tweets\\n', fontsize = 20) \n",
    "plt.ylabel(f'XXX', fontsize=XX)\n",
    "plt.xlabel(f'XXX', fontsize=XX)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5ZidL3i6-9T"
   },
   "source": [
    "Have you noticed that many words are prepositions? \n",
    "\n",
    "If we desire to determine more relevant words to know more about the context of the tweets we need to further filter the data. \n",
    "\n",
    "There are NLP packages that allow not only clustering analysis but also semantic analysis. So we can also evaluate whether the tweets have a positive or negative connotation. If you are interested you can have a look at [`NLTK`](https://www.nltk.org/api/nltk.tag.api.html).\n",
    "\n",
    "Here we will apply our own word filter that includes prepositions and other words that will help to have a nicer plot. \n",
    "\n",
    "Add a couple of words to the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "QvLXwDYh6-9T",
    "outputId": "3487d468-fad7-4c03-8be8-e6c2c874102d"
   },
   "outputs": [],
   "source": [
    "word_filter = ['aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'around', 'as', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "        'beside', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'following', 'for', 'from', 'in', \n",
    "        'inside', 'into', 'like', 'minus', 'near', 'next', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', \n",
    "        'regarding', 'round', 'save', 'since', 'than', 'through', 'till', 'toward', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', \n",
    "        'via', 'with', 'within', 'without', 'this', 'rt', 'can', 'have', 'if', 'you', 'i', 'some', 'was', 'we', 'our', 'be', 'who',\n",
    "        'this', 'rt', 'can', 'have', 'if', 'you', 'i', 'some', 'was', 'we', 'our', 'be', 'who', 'is', 'my', '–', 'are', 'know','that', 'will', 'get',\n",
    "        'make', 'your', 'more', 'so', 'don’t', 'it’s', 'i’m', 'their', 'it', 'us', 'his', 'not', 'one', 'what', 'when', 'first', 'sure', 'do', 'new', 'last',\n",
    "        'all', 'many', 'just', 'been', 'want', 'only', 'years', 'year', 'today', 'day', ...]\n",
    "\n",
    "count_filter = 5\n",
    "x = []\n",
    "h = []\n",
    "filter = []\n",
    "for i in range(len(words)):\n",
    "    if list(words.items())[i][1] > count_filter:\n",
    "        w = list(words.items())[i][0]\n",
    "        c = 0\n",
    "        for p in word_filter:\n",
    "            if w == p:\n",
    "                c = c + 1\n",
    "\n",
    "        if c == 0:\n",
    "            x.append(list(words.items())[i][0])\n",
    "            h.append(list(words.items())[i][1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5)) \n",
    "plt.bar(x, h)\n",
    "plt.xticks(fontsize = XX, rotation = XX)\n",
    "plt.yticks(fontsize = XX)\n",
    "plt.title(f'The most used words by {screen_name} in the last 200 tweets\\nUsing a word filter', fontsize = 20) \n",
    "plt.ylabel(f'XXX', fontsize=16)\n",
    "plt.xlabel(f'XXX', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4eTSnO96-9T"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 2:</b> Upload the filtered figure of the most used words by @BarackObama in the last 200 tweets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BbAA1f36-9T"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 3:</b> Which is the word that he uses the most? What is the context/tone of his words? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stcl1l8h6-9U"
   },
   "source": [
    "## 4. Data retrieval and post-processing\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Hf-O3G6-9U"
   },
   "source": [
    "We've learned how to retrieve the tweets of a specific user. \n",
    "Now it is time to retrieve them by using keywords that can be content in the text such as hashtags. \n",
    "That will allow us to analyze what is happening at a specific time and/or location. \n",
    "\n",
    "The following data contains 100 filtered tweets that contain the word \"Earthquake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_OlpBK26-9U"
   },
   "outputs": [],
   "source": [
    "tweets_file = os.path.join(data_path, r'Earthquakes_tweets.jsonl') \n",
    "\n",
    "with open(tweets_file) as f:\n",
    "    tweets = [json.loads(line) for line in f]\n",
    "\n",
    "list_tweets = tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SLi_6iZ6-9U"
   },
   "source": [
    "You can see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fnx5rRif6-9U",
    "outputId": "d71e2471-e6cc-4be9-b7b8-bc673f3b588a"
   },
   "outputs": [],
   "source": [
    "list_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjhN-vrY6-9U"
   },
   "source": [
    "As you may have noticed, it is difficult to read the tweets, and therefore, in order to do an analysis we first need to process the data. \n",
    "\n",
    "We know how to extract specific fields from the data, it would be handy to create then a DataFrame with the information we need.\n",
    "\n",
    "The following function uses the list of tweets to create a DataFrame, extract the information of each tweet, and finally add it to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3D8_ad-o6-9U"
   },
   "outputs": [],
   "source": [
    "def DataFrame_tweets(list_tweets):\n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                'description',\n",
    "                                'date_time',\n",
    "                                'location',\n",
    "                                'following',\n",
    "                                'followers',\n",
    "                                'totaltweets',\n",
    "                                'retweetcount',\n",
    "                                'text',\n",
    "                                'hashtags'])\n",
    "\n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet['screen_name']\n",
    "                description = tweet['description']\n",
    "                date_time = tweet['created_at']\n",
    "                location = tweet['location']\n",
    "                following = tweet['friends_count']\n",
    "                followers = tweet['followers_count']\n",
    "                totaltweets = tweet['statuses_count']\n",
    "                retweetcount = tweet['retweet_count']\n",
    "                text = tweet['text']\n",
    "                hashtags = tweet['hashtags']\n",
    "             \n",
    "                                \n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [username, description, date_time,\n",
    "                        location, following, \n",
    "                        followers, totaltweets,\n",
    "                        retweetcount, text, hashtags]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA4Pv8zz6-9U"
   },
   "source": [
    "Let's see what our DataFrame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "eZI8KSyM6-9V",
    "outputId": "25f72e6b-2533-43da-c6b7-b44ca759931b"
   },
   "outputs": [],
   "source": [
    "db = DataFrame_tweets(list_tweets)\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-GS3HPY6-9V"
   },
   "source": [
    "Please take a minute to look at the information about the location.\n",
    "\n",
    "As you may notice, not all the users share the location, and some of the users that do share it, do not necessarily use a real location. \n",
    "\n",
    "We can also obtain the geographical location by coordinates, let's try and find out if there is more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjkY7BAF6-9V"
   },
   "outputs": [],
   "source": [
    "def DataFrame_tweets_coordinates(list_tweets):\n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                'description',\n",
    "                                'date_time',\n",
    "                                'location',\n",
    "                                'Coordinates',\n",
    "                                'following',\n",
    "                                'followers',\n",
    "                                'totaltweets',\n",
    "                                'retweetcount',\n",
    "                                'text',\n",
    "                                'hashtags'])\n",
    "\n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet['screen_name']\n",
    "                description = tweet['description']\n",
    "                date_time = tweet['created_at']\n",
    "                location = tweet['location']\n",
    "                coordinates = tweet['coordinates']\n",
    "                following = tweet['friends_count']\n",
    "                followers = tweet['followers_count']\n",
    "                totaltweets = tweet['statuses_count']\n",
    "                retweetcount = tweet['retweet_count']\n",
    "                text = tweet['text']\n",
    "                hashtags = tweet['hashtags']\n",
    "\n",
    "                                \n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [username, description, date_time,\n",
    "                        location, coordinates, following,\n",
    "                        followers, totaltweets,\n",
    "                        retweetcount, text, hashtags]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "2h1TJgDy6-9V",
    "outputId": "d17b5a66-78d6-4e33-9143-ea3b80200d62"
   },
   "outputs": [],
   "source": [
    "db = DataFrame_tweets_coordinates(list_tweets)\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_GOnqy96-9V"
   },
   "source": [
    "Let's count the tweets that contain the geographical location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gk383PdC6-9V",
    "outputId": "7c212e7f-7503-4a83-c93b-175383eb9f7e"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(db)):\n",
    "    if db.Coordinates[i] != None:\n",
    "        count += 1\n",
    "\n",
    "print(f'{xxx} tweets contain the coordinates out of the 100 retrieved tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk45g2N76-9V"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 4:</b> How many tweets contain the coordinates? \n",
    "Mention at least one advantage and one disadvantage of not sharing the coordinates.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T_jovgf6-9V"
   },
   "source": [
    "Unfortunately not all (or none) of the users share the real location nor allow the geolocation for the coordinates.\n",
    "\n",
    "Therefore, when we want to analyze a certain region we can't use all the tweets and we need to further filter the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ixlqH-o6-9W"
   },
   "source": [
    "Open the previously retrieved dataset, it contains only the tweets with coordinates::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBXFnioS6-9W"
   },
   "outputs": [],
   "source": [
    "db_file = os.path.join(data_path, r'Earthquakes_wc_db.csv') \n",
    "db = pd.read_csv(db_file, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "0LJDs0sy6-9X",
    "outputId": "492dffa8-af7f-4bbb-8f78-2c73a9ed0ff8"
   },
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFMNS5a76-9Y"
   },
   "source": [
    "## 5. Application: Natural Hazards\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr4Jc1_S6-9Y"
   },
   "source": [
    "### Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhL62dUK6-9Y"
   },
   "source": [
    "In our previous section, we started filtering the tweets by keywords and location. \n",
    "\n",
    "We'll continue with the earthquake example.\n",
    "\n",
    "Did you notice there is a user that uses the [USGS](https://www.usgs.gov/programs/earthquake-hazards/earthquakes) as a source of its tweets?\n",
    "\n",
    "That's right, it is 'everyEarthquake'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbONsJ956-9Y"
   },
   "source": [
    "Let's use the last 100 posts made by @everyEarthquake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_QOvy5Q6-9Y"
   },
   "outputs": [],
   "source": [
    "db_file = os.path.join(data_path, r'everyEarthquake_db.csv') \n",
    "db = pd.read_csv(db_file, delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c08Ld5lS6-9Y"
   },
   "source": [
    "We can use GeoPandas to plot the location of the last 100 earthquakes. \n",
    "\n",
    "First, we will convert our DataFrame to a GeoDataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PVhZ7DYB6-9Y",
    "outputId": "aa5d476a-0f28-4ac7-fc2a-89a9980c3719"
   },
   "outputs": [],
   "source": [
    "dbg = gpd.GeoDataFrame(db, geometry=gpd.points_from_xy(db.X, db.Y))\n",
    "dbg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bua4WRmf6-9Y"
   },
   "source": [
    "Now let's see what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "-tgoJ9c36-9Y",
    "outputId": "2db789d5-c5a8-4b60-a495-ca865ca0b781"
   },
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "world.boundary.plot(ax=ax, color='xxx', alpha= xx)\n",
    "dbg.plot(ax=ax, marker='o', color='xxxx')\n",
    "\n",
    "\n",
    "ax.set_title('XXXX', fontsize=XX)\n",
    "ax.set_ylabel('XXX', fontsize = XX, rotation = XX)\n",
    "ax.set_xlabel('XXX', fontsize = XX)\n",
    "plt.xticks(fontsize = XX)\n",
    "plt.yticks(fontsize = XX)\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'XXX', markerfacecolor='XX', markersize=10)]\n",
    "plt.legend(handles=legend_elements, fontsize=16, bbox_to_anchor=(0.85,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHeWdwkl6-9Z"
   },
   "source": [
    "Have you noticed that the text contains information about the magnitude of earthquakes?\n",
    "\n",
    "We can also indicate the magnitude in our plot. \n",
    "\n",
    "We need to process the information to extract the values from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2l_uDW-H6-9Z",
    "outputId": "1f1e719a-3816-4597-a7e2-a48435c34283"
   },
   "outputs": [],
   "source": [
    "dbg['magnitude'] = 0\n",
    "for i in range(len(db)):\n",
    "    text = dbg.text.loc[i]\n",
    "    text_split= text.split(\" \")\n",
    "    Mag = float(text_split[3].replace('M', ''))\n",
    "    dbg['magnitude'][i] = Mag\n",
    "    \n",
    "dbg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYtparro6-9Z"
   },
   "source": [
    "Now let's see what the plot looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "L6C9aA0S6-9Z",
    "outputId": "76d3dbc0-db35-4f5a-f84e-707edd494ee6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot size of the circle\n",
    "z = dbg.magnitude\n",
    "\n",
    "#Plot color of the circle\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "world.boundary.plot(ax=ax, color='xxxx', alpha=xxx)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "dbg.plot('magnitude', ax=ax, marker='o', markersize=z*30, edgecolors='k' , cmap='YlOrRd',\n",
    "         vmin=0, vmax=8, zorder=2, legend=True, \n",
    "         legend_kwds={'label': f\"Magnitude\", 'orientation': \"vertical\"}, cax=cax) \n",
    "\n",
    "ax.set_title('XXX', fontsize=22) # set title\n",
    "ax.set_ylabel('XXX', fontsize = XX, rotation = XX)\n",
    "ax.set_xlabel('XXX', fontsize = XX)\n",
    "plt.xticks(fontsize = XX)\n",
    "plt.yticks(fontsize = XX)\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Earthquake', markerfacecolor='w', markeredgecolor='k', markersize=10)]\n",
    "plt.legend(handles=legend_elements, fontsize=16, bbox_to_anchor=(-8,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3zacHxT6-9Z"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 5:</b> Upload the map showing the reported earthquakes, showing the magnitude of the events.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNoYnN6d6-9Z"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 6:</b> Where and when happen the earthquake with the highest magnitude? Which is its magnitude?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMzqGbzR6-9Z"
   },
   "source": [
    "We can have a look at the map from the [USGS website](https://earthquake.usgs.gov/earthquakes/map/?extent=-68.0733,-194.23828&extent=77.15716,199.51172&listOnlyShown=true).* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chfa1TuV6-9Z"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 7:</b> Do you see differences from the figure that you have generated?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3fFfRbr6-9Z"
   },
   "source": [
    "As we have mentioned, users do not always share the location or the coordinates. \n",
    "\n",
    "However, there are other applications where we can still use the tweets without the location. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 8:</b> Can you think of examples of other applications? Mention at least one application that needs coordinates and one that does not need coordinates.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILHVfE2A6-9a"
   },
   "source": [
    "### Flooding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLIJ7R5c6-9a"
   },
   "source": [
    "Here we have an example of floods. This time we will use a database that has been already downloaded. \n",
    "The database contains tweets about floods located in Texas from 30/07/2014 to 15/11/2022. We can read the data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLr_xm5O6-9a"
   },
   "outputs": [],
   "source": [
    "Flood_file = os.path.join(data_path, r'Floods_tweets.jsonl') \n",
    "\n",
    "with open(Flood_file) as f:\n",
    "    tweets = [json.loads(line) for line in f]\n",
    "for tweet in tweets:\n",
    "    tweet['text'] = tweet['text'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FrU2T_s6-9a"
   },
   "source": [
    "Let's see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qit-SuyB6-9a",
    "outputId": "bdc4d460-40a5-4c26-82a9-7d7de1c99206"
   },
   "outputs": [],
   "source": [
    "for tweet in tweets[:10]:\n",
    "    print(tweet['date'], '-', tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G79Fxgoe6-9a"
   },
   "source": [
    "We can also plot this data as a bar plot to identify the days when more tweets have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cI1l5nd76-9a"
   },
   "outputs": [],
   "source": [
    "START_DATE = date(2014, 7, 30)\n",
    "END_DATE = date(2022, 11, 15)\n",
    "\n",
    "def plot_tweets(tweets, title):\n",
    "    dates = [tweet['date'] for tweet in tweets]\n",
    "    dates = [datetime.fromisoformat(date) for date in dates]\n",
    "    plt.figure(figsize=(10, 5)) \n",
    "    plt.hist(dates, range=(START_DATE, END_DATE), bins=(END_DATE - START_DATE).days)\n",
    "    plt.xticks(fontsize = XX)\n",
    "    plt.yticks(fontsize = XX)\n",
    "    plt.title(f'{title}', fontsize = XX) \n",
    "    plt.ylabel(f'Count', fontsize=XX)\n",
    "    plt.xlabel(f'Date', fontsize=XX)\n",
    "    legend_elements = [Line2D([0], [0], color='b', label=f'Tweets')]\n",
    "    plt.legend(handles=legend_elements, fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "R-vplMol6-9a",
    "outputId": "ff9992ce-9e7c-4bd1-ea8e-b1850056f682"
   },
   "outputs": [],
   "source": [
    "plot_tweets(tweets, 'Flood histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNvQnNyX6-9a"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 9:</b> Upload the flood histogram.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKsCHuH56-9a"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 10:</b> When does the high peak usually happen each year? What could be a potential explanation for the pattern?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arx4PT6X6-9b"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 11:</b> When does the bigger peak occur? What was the cause of it?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVdee64r6-9b"
   },
   "source": [
    "In Natural Language Process (NLP), semantic analysis is the process of understanding the meaning and interpretation of words. \n",
    "\n",
    "This time we can use keywords to filter the tweets, identifying negative or positive meanings.\n",
    "\n",
    "We're starting with negative words such as 'cry' and 'warning':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "s-2CjTro6-9b",
    "outputId": "13e45b67-1131-4de7-c1b0-4d31603e1eb1"
   },
   "outputs": [],
   "source": [
    "negative_keywords = ['cry', 'warning']\n",
    "filtered_tweets = []\n",
    "for tweet in tweets:\n",
    "    if not any(keyword in tweet['text'] for keyword in negative_keywords):\n",
    "        filtered_tweets.append(tweet)\n",
    "\n",
    "print(len(tweets))\n",
    "print(len(filtered_tweets))\n",
    "plot_tweets(filtered_tweets, 'XXX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDFzTV5G6-9b"
   },
   "source": [
    "Now let's try positive keywords such as 'emergency' and 'rescue':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "U97Po_fi6-9b",
    "outputId": "fb10d34a-b238-40a2-b762-688f64aff289"
   },
   "outputs": [],
   "source": [
    "positive_keywords = ['emergency', 'rescue']\n",
    "filtered_tweets = []\n",
    "for tweet in tweets:\n",
    "    if any(keyword in tweet['text'] for keyword in positive_keywords):\n",
    "        filtered_tweets.append(tweet)\n",
    "\n",
    "print(len(tweets))\n",
    "print(len(filtered_tweets))\n",
    "plot_tweets(filtered_tweets, 'XXX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7-acSWF6-9b"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 12:</b> Can you think about other negative keywords? Can you think of other positive keywords?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lecture Outline",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "ad67672930c25b4e013d711121a8401d6e445a01ee16ba39e2701f0a12a7bf96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
