{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnMAMUx66-9C"
      },
      "source": [
        "# Tutorial 1: Social Media & Natural Hazards\n",
        "\n",
        "Natural processes such as thunderstorms, wildfires, earthquakes, and floods may lead to significant losses in terms of property and human life. Gathering information about the damages in time is crucial and may help in mitigating the loss, and faster recovery ([Said et al, 2019](https://link.springer.com/article/10.1007/s11042-019-07942-1)).\n",
        "\n",
        "Social media are one of the most important sources of not only real-time information but records since their existence. They have been crawled over the years to collect and analyze disaster-related multimedia content ([Said et al, 2019](https://link.springer.com/article/10.1007/s11042-019-07942-1)). There are different applications where we can use social media data to analyze natural disasters.\n",
        "\n",
        "Through this tutorial, we will learn how we can use Twitter data to analyze natural hazards. We will do so by applying the concept of natural language processing.\n",
        "\n",
        "\n",
        "\n",
        "This tutorial is heavily based upon the work of [others](https://www.jcchouinard.com/tweepy-basic-functions/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfI4fylf6-9F"
      },
      "source": [
        "### Important before we start\n",
        "---\n",
        "Make sure that you save this file before you continue, else you will lose everything. To do so, go to **Bestand/File** and click on **Een kopie opslaan in Drive/Save a Copy on Drive**!\n",
        "\n",
        "Now, rename the file into Week6_Tutorial1.ipynb. You can do so by clicking on the name in the top of this screen.\n",
        "\n",
        "By using this notebook and associated files, you agree to the Twitter Developer Agreement and Policy, which can be found [here](https://developer.twitter.com/en/developer-terms/agreement-and-policy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR69C3_d6-9I"
      },
      "source": [
        "## Learning Objectives\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFosroWc6-9J"
      },
      "source": [
        "- Learn about the importance and application of social media data\n",
        "- Access social media (Twitter) through the API\n",
        "- Retrieve Twitter data\n",
        "- Filter and clean the retrieved data\n",
        "- Visualize the data in different plots such as `bar`, `scatter`, and `spatial`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXY3uirp6-9K",
        "toc": true
      },
      "source": [
        "<h2>Tutorial outline<span class=\"tocSkip\"></span></h2>\n",
        "<hr>\n",
        "<div class=\"toc\"><ul class=\"toc-item\">\n",
        "    <li><span><a href=\"#introducing-the-packages\" data-toc-modified-id=\"1.-Introduction-1\">1. Introducing the packages</a></span></li>\n",
        "    <li><span><a href=\"#social-media\" data-toc-modified-id=\"2.-Basic-Python-Data-Types-2\">2. Social Media </a></span></li>\n",
        "    <li><span><a href=\"#natural-language-processing-(nlp)\" data-toc-modified-id=\"3.-Lists-and-Tuples-3\">3. Natural Language Processing (NLP)</a></span></li>\n",
        "    <li><span><a href=\"#data-retrieval-and-post-processing\" data-toc-modified-id=\"4.-String-Methods-4\">4. Data retrieval and post-processing </a></span></li>\n",
        "    <li><span><a href=\"#applications:-detecting-natural-hazards\" data-toc-modified-id=\"5.-Dictionaries-5\">5. Applications: detecting natural hazards</a></span></li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuLiygDe6-9K",
        "tags": []
      },
      "source": [
        "## 1.Introducing the packages\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVsafgLj6-9K"
      },
      "source": [
        "Within this tutorial, we are going to make use of the following packages:\n",
        "\n",
        "[**GeoPandas**](https://geopandas.org/) is a Python packagee that extends the datatypes used by pandas to allow spatial operations on geometric types.\n",
        "\n",
        "[**JSON**](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax.\n",
        "\n",
        "[**Matplotlib**](https://matplotlib.org/) is a comprehensive Python package for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.\n",
        "\n",
        "[**NLTK**](https://www.nltk.org/) is a platform for building Python programs to work with human language data.\n",
        "\n",
        "[**NumPy**](https://numpy.org/doc/stable/) is a Python library that provides a multidimensional array object, various derived objects, and an assortment of routines for fast operations on arrays.\n",
        "\n",
        "[**Pandas**](https://pandas.pydata.org/docs/) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
        "\n",
        "[**wordcloud**](https://pypi.org/project/wordcloud/) is a little word cloud generator in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLuc04_6-9M"
      },
      "source": [
        "Now we will import these packages in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBTL4t5A6-9N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pylab as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "import geopandas as gpd\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luWgRZ9V6-9N"
      },
      "source": [
        "## 2. Social Media\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpSH17J6-9N"
      },
      "source": [
        "Social media are interactive technologies that facilitate the creation and sharing of information, ideas, interests, and other forms of expression through virtual communities and networks.\n",
        "\n",
        "Therefore, social media can be used as a source of real-time information for natural disaster detection. Moreover, the database can be used to post-analyze natural disasters for a better estimation of the extent and the damages the hazard had caused.\n",
        "\n",
        "Some of the most popular social media websites, with more than 100 million registered users, include Facebook (and its associated Facebook Messenger), TikTok, WeChat, ShareChat, Instagram, QZone, Weibo, Twitter, Tumblr, Baidu Tieba, and LinkedIn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB_Zsdwm6-9O"
      },
      "source": [
        "Twitter has been proven to be a useful data source for many research communities (Ekta et al, 2017, Graff et al, 2022), from social science to computer science, it can advance research objectives on topics as diverse as the global conversations happening on Twitter. It is one of the most popular online social networking sites with around 450 million monthly [active users](https://www.demandsage.com/twitter-statistics/) as of 2022. An important characteristic of Twitter is its real-time nature.\n",
        "\n",
        "Twitter offers tools and programs that help people when emergencies and natural disasters strike, allowing channels of communication and humanitarian response, among other [areas of focus](https://about.twitter.com/en/who-we-are/twitter-for-good) such as environmental conservation and sustainability.\n",
        "\n",
        "The [Twitter API](https://developer.twitter.com/en) enables programmatic access to Twitter in unique and advanced ways. Twitter's Developer Platform enables you to harness the power of Twitter's open, global, real-time, and historical platform within your own applications. The platform provides tools, resources, data, and API products for you to integrate, and expand Twitter's impact through research, solutions, and more.\n",
        "\n",
        "Unfortunately, Twitter has made their API policy very strict since February 12. As such, we cannot use their API within this tutorial. As such, we will use data that was previously retrieved by us (in the preparation of this tutorial).\n",
        "\n",
        "Download the 'Week6_Data' folder provided in Canvas and save it to your previously created BigData folder on your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6PYNLTR6-9O"
      },
      "source": [
        "### Connect to google drive\n",
        "<hr>\n",
        "\n",
        "To be able to read the data from Google Drive, we need to *mount* our Drive to this notebook.\n",
        "\n",
        "As you can see in the cell below, make sure that in your **My Drive** folder, where you created **BigData** folder and within that folder, you have created a **Week6_Data** folder in which you can store the files that are required to run this analysis.\n",
        "\n",
        "Please go the URL when its prompted in the box underneath the following cell, and copy the authorization code in that box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnI0VgHY6-9O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "sys.path.append(\"/content/gdrive/My Drive/BigData/Week6_Data\")\n",
        "\n",
        "data_path = os.path.join('/content/gdrive/My Drive/BigData','Week6_Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22kJ7CZt6-9P"
      },
      "source": [
        "#### User Information\n",
        "\n",
        "Trough Twitter API we can make requests such as getting the information of a user, for example, we can show the followers of a user or list the user's latest post.\n",
        "\n",
        "The following line will open the previously retrieved information of the [IVM - VU](https://twitter.com/VU_IVM) account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2zXZEYB6-9P"
      },
      "outputs": [],
      "source": [
        "user_file = os.path.join(data_path, r'VU_user.jsonl')\n",
        "\n",
        "f = open(user_file)\n",
        "user = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyUHLlHL6-9P"
      },
      "source": [
        "Have look at the information provided by Twitter for a specific user using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXUkM9Ms6-9P"
      },
      "outputs": [],
      "source": [
        "user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6O7JFNu6-9P"
      },
      "source": [
        "We can also look at a specific field of the data (e.g. name, location, etc.).\n",
        "\n",
        "Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgTjRy6A6-9P"
      },
      "outputs": [],
      "source": [
        "print(f\"user.name: {user['name']}\")\n",
        "print(f\"user.screen_name: {user['screen_name']}\")\n",
        "print(f\"user.location: {user['XXXX']}\") # change the XXXX for the field we want to print\n",
        "print(f\"user.description: {user['XXXX']}\")\n",
        "print(f\"user.followers_count: {user['XXXX']}\")\n",
        "print(f\"user.listed_count: {user['listed_count']}\")\n",
        "print(f\"user.statuses_count: {user['statuses_count']}\")\n",
        "print(f\"user urls: {user['entities']['url']['urls'][0]['expanded_url']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CW0TSdD6-9Q"
      },
      "source": [
        "Now let's see the latest 5 tweets the IVM has posted. We will use the data that was previously retrieved:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPgNT6xE6-9Q"
      },
      "outputs": [],
      "source": [
        "tweets_file = os.path.join(data_path, r'VU_tweets.jsonl')\n",
        "\n",
        "with open(tweets_file) as f:\n",
        "    tweets = [json.loads(line) for line in f]\n",
        "\n",
        "Latest_tweets = tweets[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTadNR826-9Q"
      },
      "source": [
        "Have you tried to see what the data looks like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DM-o2DM6-9Q"
      },
      "outputs": [],
      "source": [
        "Latest_tweets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtE8VMP6-9Q"
      },
      "source": [
        "As you can see, data contains many parameters and its format is not that convenient for analyzing it.\n",
        "\n",
        "Later we will learn how to create a DataFrame and process the data. For now, some important parameters we can extract from the tweet data are the description, location, text, hashtags, among others.\n",
        "\n",
        "Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYVH6gG06-9Q"
      },
      "outputs": [],
      "source": [
        "print(f\"username: {Latest_tweets[0]['screen_name']}\")\n",
        "print(f\"description: {Latest_tweets[0]['XXXX']}\")# change the XXXX for the field we want to print\n",
        "print(f\"text: {Latest_tweets[0]['XXXX']}\")\n",
        "print(f\"date_time: {Latest_tweets[0]['XXXX']}\")\n",
        "print(f\"location: {Latest_tweets[0]['XXXX']}\")\n",
        "print(f\"coordinates: {Latest_tweets[0]['XXXX']}\")\n",
        "print(f\"following: {Latest_tweets[0]['friends_count']}\")\n",
        "print(f\"followers: {Latest_tweets[0]['followers_count']}\")\n",
        "print(f\"totaltweets: {Latest_tweets[0]['statuses_count']}\")\n",
        "print(f\"retweetcount: {Latest_tweets[0]['retweet_count']}\")\n",
        "print(f\"hashtags: {Latest_tweets[0]['hashtags']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHgjAL5y6-9R"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 1:</b> How many followers does @VU_IVM have?\n",
        "When was the last time @VU_IVM tweeted/retweeted?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRUnCR8L6-9R"
      },
      "source": [
        "We've now learned how to make Twitter API requests (albeit unconnected) to get some information from a specific user.\n",
        "\n",
        "Please note that we can also make changes to our own account such as updating our profile and interacting with other users. If you're enthusiastic about it, you can find more information [here](https://www.jcchouinard.com/tweepy-basic-functions/) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k1lOWrF6-9R"
      },
      "source": [
        "## 3. Natural Language Processing (NLP)\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VPhByiq6-9R"
      },
      "source": [
        "As mentioned in [Lecture](https://elcok.github.io/BigData_AED/week6/lecture.html), Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interaction between computers and humans in natural language. The ultimate goal of NLP is to help computers understand language as well as we do.\n",
        "There are two ways of understanding natural language: Syntactic and Semantic analysis. Whereas Syntactic analysis (also referred to as syntax analysis or parsing) is the process of analyzing natural language with the rules of formal grammar, Semantic analysis is the process of understanding the meaning and interpretation of words, signs, and sentence structure.\n",
        "\n",
        "There are different techniques for understanding text such as Parsing, Stemming, Text Segmentation, Named Entity Recognition, Relationship Extraction, and Sentiment Analysis (see [Lecture](https://elcok.github.io/BigData_AED/week6/lecture.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTUZhhXd8I4o"
      },
      "source": [
        "In this section, we'll utilize the NLTK package to tokenize and normalize our corpus. Tokenization involves breaking down text into individual words or tokens, while normalization ensures uniformity by standardizing text formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hArCm7Ig8RZ_"
      },
      "source": [
        "### 3.1 Downloading Punctuation and Wordnet Corpus\n",
        "\n",
        "Before we dive into tokenization and normalization, let's ensure we have the necessary resources. We'll begin by downloading the punctuation corpus and the WordNet lexical database, which will be useful for later stages of text processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxc8k4xYo6n2"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gRJ4jsj8jGM"
      },
      "source": [
        "By executing the above code, NLTK will acquire the required resources, essential for accurate tokenization and further linguistic analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrlKxNc08mBo"
      },
      "source": [
        "### 3.2 Understanding Corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NO_s6ablnMG"
      },
      "source": [
        "**Corpus** (literally Latin for body) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDQfZZ3PiHYg"
      },
      "outputs": [],
      "source": [
        "example_sent = \"\"\"This is a sample sentence, showing off the stop words filtration. We will also show a sample word cloud\"\"\"\n",
        "print(example_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp0ojK278zXz"
      },
      "source": [
        "### 3.3 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1oJd7halhSl"
      },
      "source": [
        "**Tokenization** is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3h5puCNiHd4"
      },
      "outputs": [],
      "source": [
        "word_tokens = word_tokenize(example_sent)\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVCus8kv86CA"
      },
      "source": [
        "### 3.4 Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJE4dWNl69E"
      },
      "source": [
        "**Normalization** generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONgfKsPpJOx"
      },
      "outputs": [],
      "source": [
        "# Initialize a tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "normalized_sentence = tokenizer.tokenize(str(XXXX)) # Change the XXXX for the sentence you want to normalize\n",
        "print(f\"Normalized sentence: {normalized_sentence}\")\n",
        "print(f\"Length: {XXXX}\") # Print the length of the normalized sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLwmCXGU8-RL"
      },
      "source": [
        "### 3.5 Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDe2tJIAlZUG"
      },
      "source": [
        "**Stop words** are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, “the,” “and,” and “a,” while all required words in a particular passage, don’t generally contribute greatly to one’s understanding of content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4ezb-ivhyvk"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZZLncw9PFD"
      },
      "source": [
        "By executing the above code, NLTK will acquire the required resources.\n",
        "\n",
        "Now, let's acquire the stop words for English and have a look at them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVo_kjJ7hy5m"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wquf7sn69beK"
      },
      "source": [
        "The following line will convert words in word_tokens to lowercase and filter out stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "096OIOUqiHib"
      },
      "outputs": [],
      "source": [
        "filtered_sentence = [w for w in normalized_sentence if not w.lower() in stop_words]\n",
        "print(len(filtered_sentence)), print(filtered_sentence)\n",
        "print(f\"Filtered sentence: {filtered_sentence}\")\n",
        "print(f\"Length: {XXXX}\") # Print the length of the filtered sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzw7tmZH-aac"
      },
      "source": [
        "### 3.6 Lemmatization and Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOcVgadpmMeM"
      },
      "source": [
        "**Lemmatization** is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word’s lemma. For example, stemming the word “better” would fail to return its citation form (another word for lemma); however, lemmatization would result in changing better into good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFK2we6Oj1bH"
      },
      "outputs": [],
      "source": [
        "# Initialize wordnet lemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry2N6oR8mU7i"
      },
      "outputs": [],
      "source": [
        "# Lemmatize filtered words\n",
        "lemmatized = [wnl.lemmatize(word, pos=\"v\") for word in XXXX] # Change the XXXX for the sentence you want to lemmatize\n",
        "print(f\"Lemmatized sentence: {lemmatized}\")\n",
        "print(f\"Length: {XXXX}\")  # Print the length of the lematized sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggzVtyQSmQPy"
      },
      "source": [
        "**Stemming** is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq6fWPdIj1dn"
      },
      "outputs": [],
      "source": [
        "# Initialize Python porter stemmer\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRvk9d9omh9N"
      },
      "outputs": [],
      "source": [
        "# Stem filtered words\n",
        "stemmed = [ps.stem(word) for word in XXXX] # Change the XXXX for the sentence you want to stem\n",
        "print(f\"Stemmed sentence: {stemmed}\")\n",
        "print(f\"Length: {XXXX}\") # Print the length of the stemmed sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61cpwdGzmzfU"
      },
      "source": [
        "Lets see the differences between lemmatization and stemming together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OndgvtC3j1hA"
      },
      "outputs": [],
      "source": [
        "print(\"{0:20}{1:20}{2:20}\".format(\"Word\", \"Lemmatized\", \"Stemmed\"))\n",
        "\n",
        "for word in filtered_sentence:\n",
        "   print (\"{0:20}{1:20}{2:20}\".format(word, wnl.lemmatize(word, pos=\"v\"), ps.stem(word),))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd8idTR7pg1H"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 2:</b> explain the differences between the three datasets: the original word data, the lemmatized data, and the stemmed data. Additionally, in what scenarios would you choose to use each type of data? (Feel free to explore the differences by using a different sentence.)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR7Zz6Hkncpc"
      },
      "source": [
        "### 3.7 Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU-8x5dbY8b6"
      },
      "source": [
        "In the process of text analysis, we delve into the frequency of words to gain insights. This starts by distinguishing unique words within the text – those that occur only once. Once we've identified these unique terms, we proceed to quantify how frequently each one appears throughout the text. This meticulous approach allows us to grasp the significance and prominence of different words, aiding in the extraction of meaningful patterns and themes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC9wnN3_Y_11"
      },
      "source": [
        "To count the unique words in a text data, you can use Python's built-in data structures such as sets or dictionaries.\n",
        "Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVa3bQQ46-9S"
      },
      "outputs": [],
      "source": [
        "sentence = 'Big Data Analysis is really fun!'\n",
        "unique, count = np.unique(sentence.split(), return_counts=True)\n",
        "print(unique, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uoy2yvEZv8X"
      },
      "source": [
        "*np.unique()* returns two arrays, one contains the unique elements of a sentence, and the other contains the corresponding counts of each unique element.\n",
        "\n",
        "By using *dict(zip(array_1, array_2))* we can create a dictionary unique_counts where the unique elements are the keys and their counts are the values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxEU9dLF5nPi"
      },
      "source": [
        "Now, let's revisit our previous example and bring our data to life with a visually bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZZ_x0_G1oxP"
      },
      "outputs": [],
      "source": [
        "unique, count = np.unique(filtered_sentence, return_counts=True)\n",
        "unique_counts = dict(zip(unique, count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxYLefZJ1o5m"
      },
      "outputs": [],
      "source": [
        "# Make sure to change the XXX in order to have a nice plot\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.bar(unique, count)\n",
        "plt.xticks(fontsize = XX, rotation = XX)\n",
        "plt.yticks(fontsize = XX)\n",
        "plt.title(f'Words count\\n', fontsize = 20)\n",
        "plt.ylabel(f'XXX', fontsize=16)\n",
        "plt.xlabel(f'XXX', fontsize=XX)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Fdbxnra4LD"
      },
      "source": [
        "Now, let's replace our bar plot with a captivating word cloud to visualize the frequency of unique words in our dataset.\n",
        "\n",
        "A word cloud is a visual representation of text data, where the size of each word indicates its frequency or importance within the text. The more frequently a word appears in the text, the larger and more prominent it appears in the word cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143722rM07Rf"
      },
      "source": [
        "Let's create a word cloud to explore the frequency of unique words in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJDXuXEak9YT"
      },
      "outputs": [],
      "source": [
        "# Generate a WordCloud object using the unique word frequencies\n",
        "wc = WordCloud(background_color='XXXX').generate_from_frequencies(unique_counts)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RY9lOloF2-"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 3:</b> What are the advantages and disadvantages of utilizing a bar plot compared to a word cloud for visualizing textual data?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU-CtAgW_0bh"
      },
      "source": [
        "### 3.8 Example using social media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opyfmT7kcU41"
      },
      "source": [
        "Now that we've mastered the art of analyzing data, from tokenizing to visualizing, let's dive into an exciting example where we fuse social media data into our exploration.\n",
        "\n",
        "In this exercise, we'll harness the power of tweets to extract insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0_17GRZ6-9R"
      },
      "source": [
        "In 2023, Barack Obama ([@BarackObama](https://twitter.com/BarackObama)) had the Twitter account with the most followers according to [this website](https://www.tweetbinder.com/blog/top-twitter-accounts/). So let's explore the words he uses the most.\n",
        "\n",
        "We had retrieved 200 tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VHi0hf56-9R"
      },
      "outputs": [],
      "source": [
        "user_file = os.path.join(data_path, r'Obama_user.jsonl')\n",
        "\n",
        "f = open(user_file)\n",
        "user = json.load(f)\n",
        "\n",
        "tweets_file = os.path.join(data_path, r'Obama_tweets.jsonl')\n",
        "\n",
        "with open(tweets_file) as f:\n",
        "    tweets = [json.loads(line) for line in f]\n",
        "\n",
        "Latest_tweets = tweets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUtxGs9j6-9R"
      },
      "source": [
        "In Section 2, we learned how to access get the **text** of each tweet by using ['text'] so let's go further first with only one tweet.\n",
        "\n",
        "What type of data does it contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-MH6oda6-9R"
      },
      "outputs": [],
      "source": [
        "type(Latest_tweets[0]['screen_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4YKq4Ltd227"
      },
      "source": [
        "Let's commence the process of analyzing our tweets step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huv3c7Vfc8mL"
      },
      "source": [
        "1. **Corpus Creation**: We'll assemble our collection of tweets to form the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IDR5DfpdCNT"
      },
      "outputs": [],
      "source": [
        "corpus = ''\n",
        "for i in range(len(Latest_tweets)):\n",
        "    corpus = corpus +\" \"+ Latest_tweets[i]['text'].lower()\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCMJonVLeKx5"
      },
      "source": [
        "2. **Tokenization**: Next, we'll break down the text of each tweet into individual words or tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9zw42Kj6TY_"
      },
      "outputs": [],
      "source": [
        "word_tokens = word_tokenize(XXXX) # Change the XXXX for the corpus you want to tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftp4NXVReOuA"
      },
      "source": [
        "3. **Normalization**: We'll standardize the text by converting it to lowercase, removing punctuation, and performing other necessary normalization tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIhiOvK2qFUf"
      },
      "outputs": [],
      "source": [
        "normalized_sentence = tokenizer.tokenize(str(word_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_WUEHWSePog"
      },
      "source": [
        "4. **Stopwords Removal**: We'll eliminate common stopwords that add little meaning to the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0HbVLPX65Vc"
      },
      "outputs": [],
      "source": [
        "filtered_sentence = [w for w in normalized_sentence if not w.lower() in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnVTpb9XAk8D"
      },
      "source": [
        "Now, let's take a closer look at the differences we've encountered during our text processing so far:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npITJZbK6-9S"
      },
      "outputs": [],
      "source": [
        "screen_name = user['screen_name']\n",
        "print(f'User = {screen_name}')\n",
        "print(f'Corpus size = {len(corpus)}')\n",
        "print(f'Tokens size = {XXXX}') # Print the length of the tikenized sentence\n",
        "print(f'Normalized size = {XXXX}') # Print the length of the normalized sentence\n",
        "print(f'Filtered size = {XXXX}') # Print the length of the filtered sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na4CDz3dfMQB"
      },
      "source": [
        "5. **Lemmatization or Stemming**: We'll reduce words to their base form to ensure consistency and simplify analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzg0fvnQfcE4"
      },
      "source": [
        "In steps 1 to 4, we've progressed from the corpus to our filtered sentence. Now, it's time to decide on a method for reducing words. You have the option to choose between lemmatization and stemming to simplify words to their base form. Feel free to utilize the example code from section 3.6 to apply your chosen method to the filtered sentence obtained in our previous step (Step 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-_TbqvYqEH-"
      },
      "outputs": [],
      "source": [
        "# Lemmatize or stem data\n",
        "reduced = # use the example code from section 3.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1edFALGo5Ta"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 4:</b> Which method did you select for reducing words to their base form, lemmatization or stemming, and why? Please provide the lines of code you used for implementing your chosen method.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS2ACbGPfOJR"
      },
      "source": [
        "6. **Visualization**: Finally, we'll use visualizations such as word clouds or bar plots to gain insights and present our findings in a visually compelling manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duo-QkG1fOeJ"
      },
      "outputs": [],
      "source": [
        "unique, count = np.unique(reduced, return_counts=True)\n",
        "unique_counts = dict(zip(unique, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNPIBbjEhGbU"
      },
      "source": [
        "Let's start visualizing the data using a bar plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zlw1Yax7YLy"
      },
      "outputs": [],
      "source": [
        "# Make sure to change the XXX in order to have a nice plot\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.bar(unique, count)\n",
        "plt.xticks(fontsize = 16, rotation = 70)\n",
        "plt.yticks(fontsize = XX)\n",
        "plt.title(f'The most used words by {screen_name} in the retreived 200 tweets\\n', fontsize = 20)\n",
        "plt.ylabel(f'XXX', fontsize=16)\n",
        "plt.xlabel(f'XXX', fontsize=XX)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACa4tzadhhWR"
      },
      "source": [
        "Visualizing a lengthy dataset presents challenges, often resulting in plots that lack clarity and fail to provide a comprehensive overview. To address this, let's first examine the size of our dataset and explore its distribution.\n",
        "\n",
        "By determining the length of our dataset and identifying the frequency of less common words, we can gain valuable insights into the composition and distribution of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdGHYCqRiFuv"
      },
      "outputs": [],
      "source": [
        "print(f'Unique words = {len(unique)}')\n",
        "print(f'Non-frequent words = {len(count[count <= 5])}') # Filter the counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zZS_8lPjOYr"
      },
      "source": [
        "Now that we've identified that many words are infrequently used, let's apply a filter to the word counts and focus only on those words mentioned more than *X* times for our plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC4Pwirw8d9j"
      },
      "outputs": [],
      "source": [
        "count_filter = 10\n",
        "\n",
        "# Make sure to change the XXX in order to have a nice plot\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.bar(unique[count > count_filter], count[count > XXXX]) # Filter the counts\n",
        "plt.xticks(fontsize = XX, rotation = 70)\n",
        "plt.yticks(fontsize = XX)\n",
        "plt.title(f'The most used words by {screen_name} in the retreived 200 tweets\\n', fontsize = 20)\n",
        "plt.ylabel(f'XXX', fontsize=16)\n",
        "plt.xlabel(f'XXX', fontsize=XX)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVSKTyFonWpH"
      },
      "source": [
        "With our refined plot, we can observe that certain words, such as 'https' or 'co', have not been filtered out by the stopwords. To address this, we can create a custom filter to exclude these specific words from our analysis.\n",
        "\n",
        "Feel free to enhance the filter by adding any other words that you believe should be excluded from our analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Ml7Ab_7MYR"
      },
      "outputs": [],
      "source": [
        "word_filter = ['co', 'https']\n",
        "\n",
        "filtered_words = []\n",
        "filtered_count = []\n",
        "filter = []\n",
        "for i in range(len(unique_counts)):\n",
        "    if list(unique_counts.items())[i][1] > count_filter:\n",
        "        w = list(unique_counts.items())[i][0]\n",
        "        c = 0\n",
        "        for p in word_filter:\n",
        "            if w == p:\n",
        "                c = c + 1\n",
        "\n",
        "        if c == 0:\n",
        "            filtered_words.append(list(unique_counts.items())[i][0])\n",
        "            filtered_count.append(list(unique_counts.items())[i][1])\n",
        "\n",
        "# Make sure to change the XXX in order to have a nice plot\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(filtered_words, XXXX)\n",
        "plt.xticks(fontsize = XX, rotation = 70)\n",
        "plt.yticks(fontsize = XX)\n",
        "plt.title(f'The most used words by {screen_name} in the retreived 200 tweets\\n', fontsize = 20)\n",
        "plt.ylabel(f'XXX', fontsize=16)\n",
        "plt.xlabel(f'XXX', fontsize=XX)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUl6fvYbsUey"
      },
      "source": [
        "Great! Now we can observe some words that carry more meaningful context within the data.\n",
        "\n",
        "Moving forward, we'll create a word cloud using a mask. Let's begin by importing an image of the US shape and using it to create a mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9apGJB9sXwi"
      },
      "outputs": [],
      "source": [
        "#Import the image\n",
        "usa_file = os.path.join(data_path, r'usa.jpg')\n",
        "\n",
        "I = Image.open(usa_file)\n",
        "\n",
        "# Create an array from the image you want to use as a mask\n",
        "usa_mask = np.array(I)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbBhWqbTlcTs"
      },
      "source": [
        "This time, we'll create a function that reads the *data, title, *and* mask* to generate a word cloud. By using the *wordcloud* library, we can customize the appearance of the word cloud, including the style, color, size, and background color.\n",
        "\n",
        "Feel free to experiment with the style to achieve your desired visual effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8VPzuZqse4U"
      },
      "outputs": [],
      "source": [
        "# Wordcloud function\n",
        "def generate_better_wordcloud(data, title, mask=None):\n",
        "    cloud = WordCloud(scale=3,\n",
        "                      max_words=150,\n",
        "                      colormap='RdYlGn',\n",
        "                      mask=mask,\n",
        "                      background_color='white',\n",
        "                      stopwords=stopwords,\n",
        "                      collocations=True,\n",
        "                      contour_color='black',\n",
        "                      contour_width=1).generate_from_frequencies(data)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.imshow(cloud)\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy8tXSXlnV4a"
      },
      "outputs": [],
      "source": [
        "filtered_frequencies = dict(zip(filtered_words, filtered_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuSV_823nWGC"
      },
      "outputs": [],
      "source": [
        "# Use the function with the rome_corpus and our mask to create word cloud\n",
        "title = f'The most used words by {screen_name} in the retreived 200 tweets\\n'\n",
        "generate_better_wordcloud(XXXX, title, mask=usa_mask) # Make sure to use the function with the correct Data and a nice title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BbAA1f36-9T"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 5:</b> Which is the word that @BarackObama used the most? What is the context/tone of his words?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stcl1l8h6-9U"
      },
      "source": [
        "## 4. Data retrieval and post-processing\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Hf-O3G6-9U"
      },
      "source": [
        "We've learned how to retrieve the tweets of a specific user.\n",
        "Now it is time to retrieve them by using keywords that can be content in the text such as hashtags.\n",
        "That will allow us to analyze what is happening at a specific time and/or location.\n",
        "\n",
        "The following data contains 100 filtered tweets that contain the word \"Earthquake\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_OlpBK26-9U"
      },
      "outputs": [],
      "source": [
        "tweets_file = os.path.join(data_path, r'Earthquakes_tweets.jsonl')\n",
        "\n",
        "with open(tweets_file) as f:\n",
        "    tweets = [json.loads(line) for line in f]\n",
        "\n",
        "list_tweets = tweets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SLi_6iZ6-9U"
      },
      "source": [
        "You can see how the data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnx5rRif6-9U"
      },
      "outputs": [],
      "source": [
        "list_tweets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjhN-vrY6-9U"
      },
      "source": [
        "As you may have noticed, it is difficult to read the tweets, and therefore, in order to do an analysis we first need to process the data.\n",
        "\n",
        "We know how to extract specific fields from the data, it would be handy to create then a DataFrame with the information we need.\n",
        "\n",
        "The following function uses the list of tweets to create a DataFrame, extract the information of each tweet, and finally add it to the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8_ad-o6-9U"
      },
      "outputs": [],
      "source": [
        "def DataFrame_tweets(list_tweets):\n",
        "        # Creating DataFrame using pandas\n",
        "        db = pd.DataFrame(columns=['username',\n",
        "                                'description',\n",
        "                                'date_time',\n",
        "                                'location',\n",
        "                                'following',\n",
        "                                'followers',\n",
        "                                'totaltweets',\n",
        "                                'retweetcount',\n",
        "                                'text',\n",
        "                                'hashtags'])\n",
        "\n",
        "        # we will iterate over each tweet in the\n",
        "        # list for extracting information about each tweet\n",
        "        for tweet in list_tweets:\n",
        "                username = tweet['screen_name']\n",
        "                description = tweet['description']\n",
        "                date_time = tweet['created_at']\n",
        "                location = tweet['location']\n",
        "                following = tweet['friends_count']\n",
        "                followers = tweet['followers_count']\n",
        "                totaltweets = tweet['statuses_count']\n",
        "                retweetcount = tweet['retweet_count']\n",
        "                text = tweet['text']\n",
        "                hashtags = tweet['hashtags']\n",
        "\n",
        "\n",
        "                # Here we are appending all the\n",
        "                # extracted information in the DataFrame\n",
        "                ith_tweet = [username, description, date_time,\n",
        "                        location, following,\n",
        "                        followers, totaltweets,\n",
        "                        retweetcount, text, hashtags]\n",
        "                db.loc[len(db)] = ith_tweet\n",
        "        return db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA4Pv8zz6-9U"
      },
      "source": [
        "Let's see what our DataFrame looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZI8KSyM6-9V"
      },
      "outputs": [],
      "source": [
        "db = DataFrame_tweets(list_tweets)\n",
        "db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-GS3HPY6-9V"
      },
      "source": [
        "Please take a minute to look at the information about the location.\n",
        "\n",
        "As you may notice, not all the users share the location, and some of the users that do share it, do not necessarily use a real location.\n",
        "\n",
        "We can also obtain the geographical location by coordinates, let's try and find out if there is more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjkY7BAF6-9V"
      },
      "outputs": [],
      "source": [
        "def DataFrame_tweets_coordinates(list_tweets):\n",
        "        # Creating DataFrame using pandas\n",
        "        db = pd.DataFrame(columns=['username',\n",
        "                                'description',\n",
        "                                'date_time',\n",
        "                                'location',\n",
        "                                'Coordinates',\n",
        "                                'following',\n",
        "                                'followers',\n",
        "                                'totaltweets',\n",
        "                                'retweetcount',\n",
        "                                'text',\n",
        "                                'hashtags'])\n",
        "\n",
        "        # we will iterate over each tweet in the\n",
        "        # list for extracting information about each tweet\n",
        "        for tweet in list_tweets:\n",
        "                username = tweet['screen_name']\n",
        "                description = tweet['description']\n",
        "                date_time = tweet['created_at']\n",
        "                location = tweet['location']\n",
        "                coordinates = tweet['coordinates']\n",
        "                following = tweet['friends_count']\n",
        "                followers = tweet['followers_count']\n",
        "                totaltweets = tweet['statuses_count']\n",
        "                retweetcount = tweet['retweet_count']\n",
        "                text = tweet['text']\n",
        "                hashtags = tweet['hashtags']\n",
        "\n",
        "\n",
        "                # Here we are appending all the\n",
        "                # extracted information in the DataFrame\n",
        "                ith_tweet = [username, description, date_time,\n",
        "                        location, coordinates, following,\n",
        "                        followers, totaltweets,\n",
        "                        retweetcount, text, hashtags]\n",
        "                db.loc[len(db)] = ith_tweet\n",
        "        return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h1TJgDy6-9V"
      },
      "outputs": [],
      "source": [
        "db = DataFrame_tweets_coordinates(list_tweets)\n",
        "db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_GOnqy96-9V"
      },
      "source": [
        "Let's count the tweets that contain the geographical location:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk383PdC6-9V"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(len(db)):\n",
        "    if db.Coordinates[i] != None:\n",
        "        count += 1 # or count = count + 1\n",
        "\n",
        "print(f'{XXXX} tweets contain the coordinates out of the 100 retrieved tweets') # XXXX 'count'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk45g2N76-9V"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 6:</b> How many tweets contain the coordinates?\n",
        "Mention at least one advantage and one disadvantage of not sharing the coordinates.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtwdWFnNxvmj"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 7:</b> Provide an explanation of how the for loop and the count function operate.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T_jovgf6-9V"
      },
      "source": [
        "Unfortunately not all (or none) of the users share the real location nor allow the geolocation for the coordinates.\n",
        "\n",
        "Therefore, when we want to analyze a certain region we can't use all the tweets and we need to further filter the information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ixlqH-o6-9W"
      },
      "source": [
        "Open the previously retrieved dataset, it contains only the tweets with coordinates::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBXFnioS6-9W"
      },
      "outputs": [],
      "source": [
        "db_file = os.path.join(data_path, r'Earthquakes_wc_db.csv')\n",
        "db = pd.read_csv(db_file, delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LJDs0sy6-9X"
      },
      "outputs": [],
      "source": [
        "db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFMNS5a76-9Y"
      },
      "source": [
        "## 5. Application: Natural Hazards\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr4Jc1_S6-9Y"
      },
      "source": [
        "### Earthquakes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhL62dUK6-9Y"
      },
      "source": [
        "In our previous section, we started filtering the tweets by keywords and location.\n",
        "\n",
        "We'll continue with the earthquake example.\n",
        "\n",
        "Did you notice there is a user that uses the [USGS](https://www.usgs.gov/programs/earthquake-hazards/earthquakes) as a source of its tweets?\n",
        "\n",
        "That's right, it is 'everyEarthquake'.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbONsJ956-9Y"
      },
      "source": [
        "Let's use the last 100 posts made by @everyEarthquake:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_QOvy5Q6-9Y"
      },
      "outputs": [],
      "source": [
        "db_file = os.path.join(data_path, r'everyEarthquake_db.csv')\n",
        "db = pd.read_csv(db_file, delimiter=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c08Ld5lS6-9Y"
      },
      "source": [
        "We can use GeoPandas to plot the location of the last 100 earthquakes.\n",
        "\n",
        "First, we will convert our DataFrame to a GeoDataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVhZ7DYB6-9Y"
      },
      "outputs": [],
      "source": [
        "dbg = gpd.GeoDataFrame(db, geometry=gpd.points_from_xy(db.X, db.Y))\n",
        "dbg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bua4WRmf6-9Y"
      },
      "source": [
        "Now let's see what our data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tgoJ9c36-9Y"
      },
      "outputs": [],
      "source": [
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "plt.rcParams['font.size'] = '16'\n",
        "\n",
        "# Make sure to change the XXX in order to have a nice plot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
        "\n",
        "world.boundary.plot(ax=ax, color='xxx', alpha= xx)\n",
        "dbg.plot(ax=ax, marker='o', color='xxxx')\n",
        "\n",
        "ax.set_title('XXXX', fontsize=XX)\n",
        "ax.set_ylabel('XXX', fontsize = 16, rotation = XX)\n",
        "ax.set_xlabel('XXX', fontsize = 16)\n",
        "plt.xticks(fontsize = XX)\n",
        "plt.yticks(fontsize = XX)\n",
        "legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'XXX', markerfacecolor='XX', markersize=10)]\n",
        "plt.legend(handles=legend_elements, fontsize=16, bbox_to_anchor=(0.85,1), loc=\"upper left\")\n",
        "\n",
        "world.boundary.plot(ax=ax, color='k', alpha=.3)\n",
        "dbg.plot(ax=ax, marker='o', color='red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHeWdwkl6-9Z"
      },
      "source": [
        "Have you noticed that the text contains information about the magnitude of earthquakes?\n",
        "\n",
        "We can also indicate the magnitude in our plot.\n",
        "\n",
        "We need to process the information to extract the values from the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l_uDW-H6-9Z"
      },
      "outputs": [],
      "source": [
        "dbg['magnitude'] = 0\n",
        "for i in range(len(db)):\n",
        "    text = dbg.text.loc[i]\n",
        "    text_split= text.split(\" \")\n",
        "    Mag = float(text_split[3].replace('M', ''))\n",
        "    dbg['magnitude'][i] = Mag\n",
        "\n",
        "dbg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYtparro6-9Z"
      },
      "source": [
        "Now let's see what the plot looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6C9aA0S6-9Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Make sure to change the XXX in order to have a nice plot\n",
        "\n",
        "#Plot size of the circle\n",
        "z = dbg.magnitude\n",
        "\n",
        "#Plot color of the circle\n",
        "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
        "\n",
        "world.boundary.plot(ax=ax, color='xxxx', alpha=xxx)\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
        "dbg.plot('magnitude', ax=ax, marker='o', markersize=z*30, edgecolors='k' , cmap='YlOrRd',\n",
        "         vmin=0, vmax=8, zorder=2, legend=True,\n",
        "         legend_kwds={'label': f\"Magnitude\", 'orientation': \"vertical\"}, cax=cax)\n",
        "\n",
        "ax.set_title('XXX', fontsize=22) # set title\n",
        "ax.set_ylabel('XXX', fontsize = XX, rotation = XX)\n",
        "ax.set_xlabel('XXX', fontsize = XX)\n",
        "plt.xticks(fontsize = XX)\n",
        "plt.yticks(fontsize = XX)\n",
        "legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Earthquake', markerfacecolor='w', markeredgecolor='k', markersize=10)]\n",
        "plt.legend(handles=legend_elements, fontsize=16, bbox_to_anchor=(-8,1), loc=\"upper left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNoYnN6d6-9Z"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 8:</b> Where and when happen the earthquake with the highest magnitude? Which is its magnitude?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMzqGbzR6-9Z"
      },
      "source": [
        "We can have a look at the map from the [USGS website](https://earthquake.usgs.gov/earthquakes/map/?extent=-68.0733,-194.23828&extent=77.15716,199.51172&listOnlyShown=true).*\n",
        "\n",
        "You may notice disparities, as the tweets were retrieved in 2023, while the information from the USGS website is current and up-to-date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3fFfRbr6-9Z"
      },
      "source": [
        "As we have mentioned, users do not always share the location or the coordinates.\n",
        "\n",
        "However, there are other applications where we can still use the tweets without the location.\n",
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 9:</b> Can you think of examples of other applications? Mention at least one application that needs coordinates and one that does not need coordinates.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILHVfE2A6-9a"
      },
      "source": [
        "### Flooding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLIJ7R5c6-9a"
      },
      "source": [
        "Here we have an example of floods. This time we will use a database that has been already downloaded.\n",
        "The database contains tweets about floods located in Texas from 30/07/2014 to 15/11/2022. We can read the data using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLr_xm5O6-9a"
      },
      "outputs": [],
      "source": [
        "Flood_file = os.path.join(data_path, r'Floods_tweets.jsonl')\n",
        "\n",
        "with open(Flood_file) as f:\n",
        "    tweets = [json.loads(line) for line in f]\n",
        "for tweet in tweets:\n",
        "    tweet['text'] = tweet['text'].lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FrU2T_s6-9a"
      },
      "source": [
        "Let's see what the data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qit-SuyB6-9a"
      },
      "outputs": [],
      "source": [
        "for tweet in tweets[:10]:\n",
        "    print(tweet['date'], '-', tweet['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ-RDNojbzWV"
      },
      "outputs": [],
      "source": [
        "len(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G79Fxgoe6-9a"
      },
      "source": [
        "We can also plot this data as a bar plot to identify the days when more tweets have been posted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI1l5nd76-9a"
      },
      "outputs": [],
      "source": [
        "START_DATE = date(2014, 7, 30)\n",
        "END_DATE = date(2022, 11, 15)\n",
        "\n",
        "def plot_tweets(tweets, title):\n",
        "    dates = [tweet['date'] for tweet in tweets]\n",
        "    dates = [datetime.fromisoformat(date) for date in dates]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(dates, range=(START_DATE, END_DATE), bins=(END_DATE - START_DATE).days)\n",
        "    plt.xticks(fontsize = 10)\n",
        "    plt.yticks(fontsize = 10)\n",
        "    plt.title(f'{title}', fontsize = 16)\n",
        "    plt.ylabel(f'Count', fontsize=12)\n",
        "    plt.xlabel(f'Date', fontsize=12)\n",
        "    legend_elements = [Line2D([0], [0], color='b', label=f'Tweets')]\n",
        "    plt.legend(handles=legend_elements, fontsize=10)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-vplMol6-9a"
      },
      "outputs": [],
      "source": [
        "plot_tweets(XXXX, 'XXXX') # tweets, title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKsCHuH56-9a"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 10:</b> When does the high peak usually happen each year? What could be a potential explanation for the pattern?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arx4PT6X6-9b"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 11:</b> When does the bigger peak occur? What was the cause of it?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdee64r6-9b"
      },
      "source": [
        "In Natural Language Process (NLP), semantic analysis is the process of understanding the meaning and interpretation of words.\n",
        "\n",
        "This time we can use keywords to filter the tweets, identifying negative or positive meanings.\n",
        "\n",
        "We're starting with negative words such as 'cry' and 'warning':\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-2CjTro6-9b"
      },
      "outputs": [],
      "source": [
        "negative_keywords = ['cry', 'warning']\n",
        "filtered_tweets = []\n",
        "for tweet in tweets:\n",
        "    if not any(keyword in tweet['text'] for keyword in negative_keywords):\n",
        "        filtered_tweets.append(tweet)\n",
        "\n",
        "print(len(tweets))\n",
        "print(len(filtered_tweets))\n",
        "\n",
        "plot_tweets(XXXX, 'XXXX') # tweets, title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDFzTV5G6-9b"
      },
      "source": [
        "Now let's try positive keywords such as 'emergency' and 'rescue':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U97Po_fi6-9b"
      },
      "outputs": [],
      "source": [
        "positive_keywords = ['emergency', 'rescue']\n",
        "filtered_tweets = []\n",
        "for tweet in tweets:\n",
        "    if any(keyword in tweet['text'] for keyword in positive_keywords):\n",
        "        filtered_tweets.append(tweet)\n",
        "\n",
        "print(len(tweets))\n",
        "print(len(filtered_tweets))\n",
        "plot_tweets(filtered_tweets, 'XXXX') # tweets, title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7-acSWF6-9b"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Question 12:</b> Can you think about other negative keywords? Can you think of other positive keywords?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaV-PR4Rs2Rz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Lecture Outline",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "vscode": {
      "interpreter": {
        "hash": "ad67672930c25b4e013d711121a8401d6e445a01ee16ba39e2701f0a12a7bf96"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
