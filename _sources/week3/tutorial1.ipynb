{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Artificual neural networks (ANNs)\n",
    "\n",
    "You can also create content with Jupyter Notebooks. This means that you can include\n",
    "code blocks and their outputs in your book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tutorial Outline<span class=\"tocSkip\"></span></h2>\n",
    "<hr>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "<li><span><a href=\"#1.-Introducing the packages\" data-toc-modified-id=\"1.-Introducing-the-packages-2\">1. Introducing the packages</a></span></li>\n",
    "<li><span><a href=\"#2.-\" data-toc-modified-id=\"2.-One-hot-encoding-3\">2. One-hot encoding</a></span></li>\n",
    "<li><span><a href=\"#3.-\" data-toc-modified-id=\"3.-Normalizing-the-data-4\">3. Normalizing the data </a></span></li>\n",
    "<li><span><a href=\"#4.-\" data-toc-modified-id=\"4.-Building-a-linear-neural-network-model-with-1-variable-5\">4. Building a linear neural network model with 1 variable</a></span></li>\n",
    "<li><span><a href=\"#5.-\" data-toc-modified-id=\"5.-Training-a-linear-neural-network-model-with-1-variable-6\">5. Training a linear neural network model with 1 variable</a></span></li>\n",
    "<li><span><a href=\"#6.-\" data-toc-modified-id=\"6.-A-non-linear-neural-network-model-with-1-variable-7\">6. A non-linear neural network model with 1 variable </a></span></li>\n",
    "<li><span><a href=\"#7.-\" data-toc-modified-id=\"7.-A-non-linear-neural-network-model-with-multiple-variables-8\">7. A non-linear neural network model with multiple variables </a></span></li>\n",
    "<li><span><a href=\"#8.-\" data-toc-modified-id=\"8.-Evaluating-the-performance-9\">8. Evaluating the performance </a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn how to create dummy variables\n",
    "- Learn how to construct a linear ANN with one variable\n",
    "- Learn how to construct a non-linear ANN with one variable\n",
    "- Learn how to construct a non-linear ANN model with multiple variables\n",
    "- Get insights in the basic functions and options of neural networks in tensorflow keras \n",
    "- Learn how to make predictions with ANN models\n",
    "- Evaluate performance of ANN models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Introducing the packages\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this tutorial, we are going to make use of the following packages: \n",
    "\n",
    "\n",
    "\n",
    "*We will first need to install these packages in the cell below. Uncomment them to make sure we can pip install them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\",{'axes.grid' : True})\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use the package tensorflow and keras, which has been integrated in tensorflow. Tensorflow is a popular software library for deep learning applications, such as neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data. We use the data that we also used to estimate the OLS model. It includes the variables you created in the first tutorial and it deals with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(r\"https://github.com/ElcoK/BigData_AED/raw/main/week2/usadataforOLS.csv\", sep = ',', encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look at the data once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did in the last tutorial, we will drop duplicates based on the variable SERIAL. (Remember that persons from the same household have the same SERIAL number.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.drop_duplicates(subset='SERIAL', keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we drop several columns from the dataframe that are not useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.drop(['Unnamed: 0', 'YEAR', 'SAMPLE', 'HHWT', 'SERIAL','PERWT', 'RELATE', 'RELATED', 'COUNTYFIP'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop columns that are specific for an individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datann = data1.drop(['SEX', 'AGE', 'RACE', 'RACED', 'EDUC', 'EDUCD', 'EMPSTATD'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.One-hot encoding\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some categorical variables in our data? Before we can include them in the ANN, we need to first transform them into *dummy variables*. This is called one-hot encoding. A dummy variable takes the value of either 1 or 0. In our dataset, the variable REGION has nine distinct values (32, 42, 41, 33, 11, 31, 21, 22, and 12), each representing a unique region. That means that to transform the REGION variable into *dummies*, we actually need to create nine new columns in our dataset, one for each region separatly. For example, if the *sample unit* in our data (i.e., row), belows to REGION 32, in our new \"REGION_32\" column, it will have a value of 1, while all other sample units that are from a different region will have a value of 0 in the \"REGION_32\" column. We can create new dummy variables (columns) in our dataset based on REGION using the function get_dummies from the pandas package. We give the columns of the dummy variables a prefix 'REGION' after which follows the region number. Then we merge the the dataframe one_hot to the original dataframe with all the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(data1['REGION'], prefix = 'REGION_')\n",
    "print(one_hot.head())\n",
    "datann = pd.concat([datann, one_hot], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 1:</b> Are there other categorical variables in the dataset you might want to transform into dummy variables?\n",
    "If yes, you can do so below. Don't forget to drop the original variable from the dataframe, see the line below.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datann = datann.drop(['REGION'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Normalizing the data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, ANN models can make better predictions with ***scaled or normalized*** variables.\n",
    "Neural networks are sensitive to the scale of the input data, and large values can slow down \n",
    "or prevent convergence during training. By normalizing the data, we bring all the features \n",
    "to a similar scale, which can help the optimization process converge faster and better.\n",
    "The model could still converge without normalization, but normalization makes training \n",
    "more stable. \n",
    "We can for example normalize by subtracting the mean of the variable and dividing by the standard deviation. Another way to normalize is by subtracting the minimum of the variable and then dividing by the difference between the maximum and minimum of the variable. \n",
    "x_normalized = (x - mean(x)) / std(x)\n",
    "x_normalized = (x - min(x)) / (max(x) - min(x))\n",
    "We calculate the normalized variables ourselves, but we could also add a normalization layer to the neural network, which usually works more efficient than normalizing the data ourselves.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split the data in training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(datann, test_size=0.3, random_state=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first neural network model, we are going to construct a linear model with one explanantory variable, which is the age of the youngest household member, i.e. YOUNGEST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_data[['YOUNGEST']]\n",
    "y_train = training_data[['COSTENERGY']]\n",
    "\n",
    "x_test = testing_data[['YOUNGEST']]\n",
    "y_test = testing_data[['COSTENERGY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lines of code, we build the normalizing layer. The input_shape is [1,], because we only have one explanantory variable, which is a column vector. With the function adapt we apply the normalizer to the column YOUNGEST in x_train. We print the mean of YOUNGEST estimated by the normalizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearmodel_normalizer = tf.keras.layers.Normalization(input_shape=[1,], axis=None)\n",
    "linearmodel_normalizer.adapt(np.array(x_train['YOUNGEST']))\n",
    "print(linearmodel_normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Building a neural network model with 1 variable\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we are going to construct a very simple neural network. It will simulate a linear regression model with one explanantory variable. Remember that a linear regression has the following form: y = m + bx, where y is the independent variable, ENERGYCOST, x is the explanantory variable (we use YOUNGEST), m is the intercept, and b is the regression coefficient. \n",
    "In the model below, you see that the output layer keras.layers.Dense(units=1, activation='linear') has a linear activiation function and 1 unit of output (output would be the ENERGYCOST). Hence we go from 1 unit of input, the age of the YOUNGEST member in the household to 1 unit of output. \n",
    "Before the output layer, we add the normalizing layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " linearmodel = tf.keras.Sequential([\n",
    "    linearmodel_normalizer,\n",
    "    keras.layers.Dense(units=1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next line of code, we can check the structure of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary, we can see that we have three parameters in the normalization layer and 2 parameters in the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 2:</b> Does this model have hidden layers? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built the model, we have to specify how the model is trained using the compile function. We use the Adam optimizer with a learning rate of 0.05. Our loss function is the mean absolute error. When training the model, this loss function is minimized by the optimization algorithm. We also include the mean squared error in the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearmodel.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "    loss='mean_absolute_error',\n",
    "    metrics = [tf.keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Training a neural network model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the model using the function fit. In the code below, you can decide on the number of epochs, but we advise to choose a number between 15 and 40 (there is no right or wrong here), because it takes about 4 seconds per epoch to train the model. In each epoch, the neural network goes through all the training data and optimizes the parameters using the Adam optimizer. The more often the model sees the data, the more accurate the parameter estimates become. In each epoch we use 20% of the training data to validate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 25\n",
    "\n",
    "history_linearmodel = linearmodel.fit(\n",
    "    x_train['YOUNGEST'],\n",
    "    y_train['COSTENERGY'],\n",
    "    epochs=nr_epochs,\n",
    "    verbose=1, # Shows the training progress.\n",
    "    validation_split = 0.2) # Calculate validation results on 20% of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the training progress of the model. We plot the training loss and the validation loss. Recall that the loss function was specified as the mean absolute error of the model. We should see a curve going down, this is the learning process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_linearmodel.history['loss'], label='loss')\n",
    "plt.plot(history_linearmodel.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(np.arange(0,nr_epochs), np.arange(1,nr_epochs+1))\n",
    "plt.xlim(0,nr_epochs-1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 3:</b> Upload the figure above to canvas? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 4:</b> Interpret what you see in the figure. Do you think the model has been trained enough? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function get_weights() gives the estimated parameters of the neural network. The first three numbers belong to the normalization layer and the last two numbers are the weights of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " linearmodel.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare these weights to an OLS linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = datann[['COSTENERGY']]\n",
    "X = datann[[\"YOUNGEST\"]]\n",
    "X['Constant'] = 1\n",
    "\n",
    "regressionOLS = sm.OLS(Y, X)\n",
    "resultsOLS = regressionOLS.fit()\n",
    "\n",
    "print(resultsOLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 5:</b> The parameters of the neural network model are very different from the parameters in the OLS regression, why is that do you think? Do NOT include the number of epochs in your answer. Even when you have set the number of epochs to 15, the estimated parameters are close to the true model parameters.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take a look at the predictions by the model. We let the model predict on an array of x that goes from 0 till the maximum of YOUNGEST, divided in 200 equal steps. Using the function predict, we obtain the predicted ENERGYCOSTS, given by y in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(0, round(max(x_train['YOUNGEST'])), 200)\n",
    "y = linearmodel.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(datann['YOUNGEST'], datann['COSTENERGY'], label='Data')\n",
    "plt.plot(x, y, color='k', label='Predictions')\n",
    "plt.xlabel('YOUNGEST')\n",
    "plt.ylabel('COSTENERGY')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 6:</b> Can you derive the parameters of the linear regression given by y = m + bx from this plot? Give the values of m and b.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A non-linear neural network model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build a non-linear neural network model. We include two hidden layers in the model, the first one has 128 units and the second one 64 units. Relu is the activivation function in these two layers and the output layer has a linear activation function, because we deal with a regression problem. Note that linear is the default activation function, so we don't have to specify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model = keras.Sequential([\n",
    "      linearmodel_normalizer,\n",
    "      keras.layers.Dense(128, activation='relu'),\n",
    "      keras.layers.Dense(64, activation='relu'),\n",
    "      keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "nonlinear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary, we can see that in the non-linear model a large amount of parameters has to be estimated, even when we have only one explanantory variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 7:</b> The more nodes and layers an ANN model has, the more flexible it becomes, often resulting in a better model fit to the training data. That is generally something desirable, as long as the model is not overfitting... Explain with your own words what is the problem with overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile the model. We use a smaller learning rate this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.00005),\n",
    "                metrics = [tf.keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 8:</b> What does the learning rate parameter control in the model? What would happen if you choose a smaller or larger learning rate? You can use the figure depicting the loss over the epochs of the linear model in your answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 20\n",
    "history_nonlinearmodel = nonlinear_model.fit(\n",
    "    x_train['YOUNGEST'],\n",
    "    y_train['COSTENERGY'],\n",
    "    epochs=nr_epochs,\n",
    "    # Show logging.\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_nonlinearmodel.history['loss'], label='loss')\n",
    "plt.plot(history_nonlinearmodel.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(np.arange(0,nr_epochs), np.arange(1,nr_epochs+1))\n",
    "plt.xlim(0,nr_epochs-1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next we take a look at the predictions by the non-linear neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(0, round(max(x_train['YOUNGEST'])), 200)\n",
    "y = nonlinear_model.predict(x)\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(datann['YOUNGEST'], datann['COSTENERGY'], label='Data')\n",
    "plt.plot(x, y, color='k', label='Predictions')\n",
    "plt.xlabel('YOUNGEST')\n",
    "plt.ylabel('COSTENERGY')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 9:</b> What do you think of the performance of the non-linear model? Is there a big difference with the linear model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A non-linear neural network model with multiple variables\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to estimate a neural network on all variables in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_data.drop(columns = ['COSTENERGY','COSTELEC', 'COSTGAS', 'COSTFUEL'], axis = 1)\n",
    "y_train = training_data[['COSTENERGY']]\n",
    "\n",
    "x_test = testing_data.drop(columns = ['COSTENERGY','COSTELEC', 'COSTGAS', 'COSTFUEL'], axis = 1)\n",
    "y_test = testing_data[['COSTENERGY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a normalizing layer on all variables now. The input shape is the number of columns in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullmodel_normalizer = keras.layers.Normalization(input_shape=[x_train.shape[1],], axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullmodel_normalizer.adapt(np.array(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullmodel = keras.Sequential([\n",
    "      fullmodel_normalizer,\n",
    "      keras.layers.Dense(128, activation='relu'),\n",
    "      keras.layers.Dense(64, activation='relu'),\n",
    "      keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "\n",
    "fullmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the options for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullmodel.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.00002),\n",
    "                metrics = [tf.keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 25\n",
    "history_fullmodel = fullmodel.fit(\n",
    "    x_train,\n",
    "    y_train['COSTENERGY'],\n",
    "    epochs=nr_epochs,\n",
    "    # Show logging.\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning progress: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_fullmodel.history['loss'], label='loss')\n",
    "plt.plot(history_fullmodel.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(np.arange(0,nr_epochs), np.arange(1,nr_epochs+1))\n",
    "plt.xlim(0,nr_epochs-1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating the performance\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step, we take a look at the performance of the model on the testing dataset. We check the value of the mean absolute error and mean squared error of the three models using the function evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_linear = linearmodel.evaluate(x_test['YOUNGEST'], y_test['COSTENERGY'], verbose=1)\n",
    "test_loss_nonlinear = nonlinear_model.evaluate(x_test['YOUNGEST'], y_test['COSTENERGY'], verbose=1)\n",
    "test_loss_full = fullmodel.evaluate(x_test, y_test['COSTENERGY'], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question 10:</b> Interpret the results. Which model performed best? \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f323064ae63d54ed8d769390a968e914fbf7abacffc63e116cd2e04a08ed2d24"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
